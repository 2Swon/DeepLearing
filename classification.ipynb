{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Ip8CwHKRxBkmPMCx_M2XyHoNWc_J7xeW",
      "authorship_tag": "ABX9TyNcSVrEkJnSUVNrEnWT1w+r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ccb1665852543b8b4c56f98717703f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26d4a602affd4842ade4e127ac3c9cc3",
              "IPY_MODEL_9c680dfabf8041afa46eaaf9bf8a0a2f",
              "IPY_MODEL_05a05d6e5a274576990a0b72ced143d1"
            ],
            "layout": "IPY_MODEL_e5df2d0b9fe649138de54e3121c6a9ad"
          }
        },
        "26d4a602affd4842ade4e127ac3c9cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67557cfe9ebb4502b5109ed45b4d4907",
            "placeholder": "​",
            "style": "IPY_MODEL_7d3981252ba0443fbcf55fa03d7a2f4b",
            "value": "100%"
          }
        },
        "9c680dfabf8041afa46eaaf9bf8a0a2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e2534e4f37d4a44a99df979c77f3a6a",
            "max": 9912422,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11cabbc263774cf68373108cb11158fb",
            "value": 9912422
          }
        },
        "05a05d6e5a274576990a0b72ced143d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a69d907d4cdb46a3bdfb815e9862ff08",
            "placeholder": "​",
            "style": "IPY_MODEL_fb14c605884c46c7b9d8d41271a8765a",
            "value": " 9912422/9912422 [00:00&lt;00:00, 133234987.07it/s]"
          }
        },
        "e5df2d0b9fe649138de54e3121c6a9ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67557cfe9ebb4502b5109ed45b4d4907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d3981252ba0443fbcf55fa03d7a2f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e2534e4f37d4a44a99df979c77f3a6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11cabbc263774cf68373108cb11158fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a69d907d4cdb46a3bdfb815e9862ff08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb14c605884c46c7b9d8d41271a8765a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c91ee2aef854f4c9deac89c0469e425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d05a4fb512b64cc3854c3fcd3d2f1649",
              "IPY_MODEL_7a425d9c3aa4488db42c581f23528bcd",
              "IPY_MODEL_5c9b5cf24e8b46ecb07c481da31b7b0d"
            ],
            "layout": "IPY_MODEL_c33eaf835c844435986d7c14bed95b63"
          }
        },
        "d05a4fb512b64cc3854c3fcd3d2f1649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faaa222843704adea00055e5373d39e1",
            "placeholder": "​",
            "style": "IPY_MODEL_3c4c9cc131e24438af4a021a9f38dca0",
            "value": "100%"
          }
        },
        "7a425d9c3aa4488db42c581f23528bcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f8f6a6520574753ab6a9f36bab1c44b",
            "max": 28881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04394943a8c343eab49eb20dba7d26b3",
            "value": 28881
          }
        },
        "5c9b5cf24e8b46ecb07c481da31b7b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51ab5368d8b14bdf80619c078f075b00",
            "placeholder": "​",
            "style": "IPY_MODEL_9a65818b9ce946aaa17b52f50d3f6926",
            "value": " 28881/28881 [00:00&lt;00:00, 1530650.67it/s]"
          }
        },
        "c33eaf835c844435986d7c14bed95b63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faaa222843704adea00055e5373d39e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c4c9cc131e24438af4a021a9f38dca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f8f6a6520574753ab6a9f36bab1c44b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04394943a8c343eab49eb20dba7d26b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51ab5368d8b14bdf80619c078f075b00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a65818b9ce946aaa17b52f50d3f6926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7e9676c9ab847c281c8e13cbed3537d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9acd4fca5f6472c8922626115bb0240",
              "IPY_MODEL_75e3426fd15346c8ad3bcaa8c3c508c1",
              "IPY_MODEL_297b5543dc524da1887c79cab097dc9e"
            ],
            "layout": "IPY_MODEL_783bb7faec804abe8200a1c49dc3f63d"
          }
        },
        "b9acd4fca5f6472c8922626115bb0240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2ae908e2bdc43718964ac6c45381438",
            "placeholder": "​",
            "style": "IPY_MODEL_a8518f9ae0e0473ab1b0495598899658",
            "value": "100%"
          }
        },
        "75e3426fd15346c8ad3bcaa8c3c508c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd2fe72a2c9e4bc3a35ebb8ef402d3c2",
            "max": 1648877,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c8d809a74b84c72b604a93257ba8624",
            "value": 1648877
          }
        },
        "297b5543dc524da1887c79cab097dc9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9949f2528df1445f8e36b451dca0b44f",
            "placeholder": "​",
            "style": "IPY_MODEL_59f38f0315714b31a85082bd8d5d9296",
            "value": " 1648877/1648877 [00:00&lt;00:00, 36663988.02it/s]"
          }
        },
        "783bb7faec804abe8200a1c49dc3f63d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2ae908e2bdc43718964ac6c45381438": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8518f9ae0e0473ab1b0495598899658": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd2fe72a2c9e4bc3a35ebb8ef402d3c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c8d809a74b84c72b604a93257ba8624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9949f2528df1445f8e36b451dca0b44f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59f38f0315714b31a85082bd8d5d9296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbcaad1b87d945879ea1777dcd878d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b09235a378994debb11598316807951f",
              "IPY_MODEL_b8c633a7fcbf4a3c90ec50bf951af862",
              "IPY_MODEL_a124115b007e49b08d46967fffae5239"
            ],
            "layout": "IPY_MODEL_4caa0fb084904a1f9e8a6128717f09ef"
          }
        },
        "b09235a378994debb11598316807951f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57b88e8f740544a0a3857fd6ce09f895",
            "placeholder": "​",
            "style": "IPY_MODEL_5977b7a5c6cd4e87a1e0e6a49df6850c",
            "value": "100%"
          }
        },
        "b8c633a7fcbf4a3c90ec50bf951af862": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_202c993ac1244919a795e4d626fe528b",
            "max": 4542,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_623289a7d87b48dc8d38372907053439",
            "value": 4542
          }
        },
        "a124115b007e49b08d46967fffae5239": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a9c64c5a43f4d4c8f99af7267c652d0",
            "placeholder": "​",
            "style": "IPY_MODEL_500f774a3fa14a7e8c964b98b6a3551e",
            "value": " 4542/4542 [00:00&lt;00:00, 272910.66it/s]"
          }
        },
        "4caa0fb084904a1f9e8a6128717f09ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57b88e8f740544a0a3857fd6ce09f895": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5977b7a5c6cd4e87a1e0e6a49df6850c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "202c993ac1244919a795e4d626fe528b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623289a7d87b48dc8d38372907053439": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a9c64c5a43f4d4c8f99af7267c652d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "500f774a3fa14a7e8c964b98b6a3551e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2Swon/DeepLearing/blob/main/classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFkKb9cRFtew"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torchvision import transforms, datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hyper parameter 설정"
      ],
      "metadata": {
        "id": "lLgXOAkSGM1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, datasets\n",
        "\n",
        "## 트레이닝 필요한 파라메터를 설정하기\n",
        "lr = 1e-3\n",
        "batch_size = 64\n",
        "num_epoch = 10\n",
        "\n",
        "ckpt_dir = './checkpoint'\n",
        "log_dir = './log'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "bGDhwWVXGKB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 구현"
      ],
      "metadata": {
        "id": "0LQOaLRgGKbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 네트워크를 구축하기\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5, stride=1, padding=0, bias=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, stride=1, padding=0, bias=True)\n",
        "        self.drop2 = nn.Dropout2d(p=0.5)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=320, out_features=50, bias=True)\n",
        "        self.relu1_fc1 = nn.ReLU()\n",
        "        self.drop1_fc1 = nn.Dropout2d(p=0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(in_features=50, out_features=10, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.relu1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.drop2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.relu2(x)\n",
        "\n",
        "        x = x.view(-1, 320)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1_fc1(x)\n",
        "        x = self.drop1_fc1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "8P8oAQkxGMRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 네트워크를 저장하거나 불러오는 함수 작성하기\n",
        "def save(ckpt_dir, net, optim, epoch):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "\n",
        "    torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},\n",
        "               './%s/model_epoch%d.pth' % (ckpt_dir, epoch))\n",
        "\n",
        "def load(ckpt_dir, net, optim):\n",
        "    ckpt_lst = os.listdir(ckpt_dir)\n",
        "    ckpt_lst.sort()\n",
        "\n",
        "    dict_model = torch.load('./%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n",
        "\n",
        "    net.load_state_dict(dict_model['net'])\n",
        "    optim.load_state_dict(dict_model['optim'])\n",
        "\n",
        "    return net, optim\n"
      ],
      "metadata": {
        "id": "ZOeplJH0GSh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 불러오기"
      ],
      "metadata": {
        "id": "Sh-mhrHfGIJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## MNIST 데이터 불러오기\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
        "\n",
        "dataset = datasets.MNIST(download=True, root='./', train=True, transform=transform)\n",
        "loader = DataLoader(dataset, batch_size=batch_size,shuffle=True, num_workers=0)\n",
        "\n",
        "num_data = len(loader.dataset)\n",
        "num_batch = np.ceil(num_data / batch_size)\n",
        "\n",
        "## 네트워크 설정 및 필요한 손실함수 구현하기\n",
        "net = Net().to(device)\n",
        "params = net.parameters()\n",
        "\n",
        "fn_loss = nn.CrossEntropyLoss().to(device)\n",
        "fn_pred = lambda output: torch.softmax(output, dim=1)\n",
        "fn_acc = lambda pred, label: ((pred.max(dim=1)[1] == label).type(torch.float)).mean()\n",
        "\n",
        "optim = torch.optim.Adam(params, lr=lr)\n",
        "\n",
        "writer = SummaryWriter(log_dir=log_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435,
          "referenced_widgets": [
            "0ccb1665852543b8b4c56f98717703f8",
            "26d4a602affd4842ade4e127ac3c9cc3",
            "9c680dfabf8041afa46eaaf9bf8a0a2f",
            "05a05d6e5a274576990a0b72ced143d1",
            "e5df2d0b9fe649138de54e3121c6a9ad",
            "67557cfe9ebb4502b5109ed45b4d4907",
            "7d3981252ba0443fbcf55fa03d7a2f4b",
            "7e2534e4f37d4a44a99df979c77f3a6a",
            "11cabbc263774cf68373108cb11158fb",
            "a69d907d4cdb46a3bdfb815e9862ff08",
            "fb14c605884c46c7b9d8d41271a8765a",
            "3c91ee2aef854f4c9deac89c0469e425",
            "d05a4fb512b64cc3854c3fcd3d2f1649",
            "7a425d9c3aa4488db42c581f23528bcd",
            "5c9b5cf24e8b46ecb07c481da31b7b0d",
            "c33eaf835c844435986d7c14bed95b63",
            "faaa222843704adea00055e5373d39e1",
            "3c4c9cc131e24438af4a021a9f38dca0",
            "8f8f6a6520574753ab6a9f36bab1c44b",
            "04394943a8c343eab49eb20dba7d26b3",
            "51ab5368d8b14bdf80619c078f075b00",
            "9a65818b9ce946aaa17b52f50d3f6926",
            "a7e9676c9ab847c281c8e13cbed3537d",
            "b9acd4fca5f6472c8922626115bb0240",
            "75e3426fd15346c8ad3bcaa8c3c508c1",
            "297b5543dc524da1887c79cab097dc9e",
            "783bb7faec804abe8200a1c49dc3f63d",
            "b2ae908e2bdc43718964ac6c45381438",
            "a8518f9ae0e0473ab1b0495598899658",
            "fd2fe72a2c9e4bc3a35ebb8ef402d3c2",
            "5c8d809a74b84c72b604a93257ba8624",
            "9949f2528df1445f8e36b451dca0b44f",
            "59f38f0315714b31a85082bd8d5d9296",
            "dbcaad1b87d945879ea1777dcd878d54",
            "b09235a378994debb11598316807951f",
            "b8c633a7fcbf4a3c90ec50bf951af862",
            "a124115b007e49b08d46967fffae5239",
            "4caa0fb084904a1f9e8a6128717f09ef",
            "57b88e8f740544a0a3857fd6ce09f895",
            "5977b7a5c6cd4e87a1e0e6a49df6850c",
            "202c993ac1244919a795e4d626fe528b",
            "623289a7d87b48dc8d38372907053439",
            "5a9c64c5a43f4d4c8f99af7267c652d0",
            "500f774a3fa14a7e8c964b98b6a3551e"
          ]
        },
        "id": "51dKXYxvGy9t",
        "outputId": "d724d11f-b868-4e4b-af2f-baa329a94459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/9912422 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ccb1665852543b8b4c56f98717703f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/28881 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c91ee2aef854f4c9deac89c0469e425"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1648877 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7e9676c9ab847c281c8e13cbed3537d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4542 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbcaad1b87d945879ea1777dcd878d54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training"
      ],
      "metadata": {
        "id": "yPgEGqToGTS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 트레이닝 시작하기\n",
        "for epoch in range(1, num_epoch + 1):\n",
        "    net.train()\n",
        "\n",
        "    loss_arr = []\n",
        "    acc_arr = []\n",
        "\n",
        "    for  batch, (input, label) in enumerate(loader, 1):\n",
        "        input = input.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        output = net(input)\n",
        "        pred = fn_pred(output)\n",
        "\n",
        "        optim.zero_grad()\n",
        "\n",
        "        loss = fn_loss(output, label)\n",
        "        acc = fn_acc(pred, label)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        loss_arr += [loss.item()]\n",
        "        acc_arr += [acc.item()]\n",
        "\n",
        "        print('TRAIN: EPOCH %04d/%04d | BATCH %04d/%04d | LOSS: %.4f | ACC %.4f' %\n",
        "              (epoch, num_epoch, batch, num_batch, np.mean(loss_arr), np.mean(acc_arr)))\n",
        "\n",
        "    writer.add_scalar('loss', np.mean(loss_arr), epoch)\n",
        "    writer.add_scalar('acc', np.mean(acc_arr), epoch)\n",
        "\n",
        "    save(ckpt_dir = ckpt_dir, net=net, optim=optim, epoch=epoch)\n",
        "\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIjM75yjGUpL",
        "outputId": "426f5feb-5c73-4b52-c09a-0afede722c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [

            "TRAIN: EPOCH 0009/0010 | BATCH 0447/0938 | LOSS: 0.1426 | ACC 0.9580\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0448/0938 | LOSS: 0.1429 | ACC 0.9580\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0449/0938 | LOSS: 0.1432 | ACC 0.9579\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0450/0938 | LOSS: 0.1431 | ACC 0.9579\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0451/0938 | LOSS: 0.1430 | ACC 0.9579\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0452/0938 | LOSS: 0.1430 | ACC 0.9580\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0453/0938 | LOSS: 0.1429 | ACC 0.9580\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0454/0938 | LOSS: 0.1430 | ACC 0.9579\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0455/0938 | LOSS: 0.1429 | ACC 0.9580\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0456/0938 | LOSS: 0.1427 | ACC 0.9581\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0457/0938 | LOSS: 0.1425 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0458/0938 | LOSS: 0.1424 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0459/0938 | LOSS: 0.1424 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0460/0938 | LOSS: 0.1422 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0461/0938 | LOSS: 0.1420 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0462/0938 | LOSS: 0.1421 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0463/0938 | LOSS: 0.1421 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0464/0938 | LOSS: 0.1422 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0465/0938 | LOSS: 0.1428 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0466/0938 | LOSS: 0.1428 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0467/0938 | LOSS: 0.1429 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0468/0938 | LOSS: 0.1428 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0469/0938 | LOSS: 0.1427 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0470/0938 | LOSS: 0.1426 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0471/0938 | LOSS: 0.1428 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0472/0938 | LOSS: 0.1427 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0473/0938 | LOSS: 0.1428 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0474/0938 | LOSS: 0.1430 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0475/0938 | LOSS: 0.1430 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0476/0938 | LOSS: 0.1430 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0477/0938 | LOSS: 0.1428 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0478/0938 | LOSS: 0.1427 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0479/0938 | LOSS: 0.1425 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0480/0938 | LOSS: 0.1424 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0481/0938 | LOSS: 0.1423 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0482/0938 | LOSS: 0.1423 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0483/0938 | LOSS: 0.1421 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0484/0938 | LOSS: 0.1425 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0485/0938 | LOSS: 0.1424 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0486/0938 | LOSS: 0.1423 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0487/0938 | LOSS: 0.1424 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0488/0938 | LOSS: 0.1422 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0489/0938 | LOSS: 0.1424 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0490/0938 | LOSS: 0.1422 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0491/0938 | LOSS: 0.1422 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0492/0938 | LOSS: 0.1422 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0493/0938 | LOSS: 0.1421 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0494/0938 | LOSS: 0.1424 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0495/0938 | LOSS: 0.1425 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0496/0938 | LOSS: 0.1427 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0497/0938 | LOSS: 0.1430 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0498/0938 | LOSS: 0.1429 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0499/0938 | LOSS: 0.1428 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0500/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0501/0938 | LOSS: 0.1425 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0502/0938 | LOSS: 0.1427 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0503/0938 | LOSS: 0.1425 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0504/0938 | LOSS: 0.1424 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0505/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0506/0938 | LOSS: 0.1429 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0507/0938 | LOSS: 0.1430 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0508/0938 | LOSS: 0.1429 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0509/0938 | LOSS: 0.1431 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0510/0938 | LOSS: 0.1431 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0511/0938 | LOSS: 0.1430 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0512/0938 | LOSS: 0.1435 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0513/0938 | LOSS: 0.1435 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0514/0938 | LOSS: 0.1437 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0515/0938 | LOSS: 0.1438 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0516/0938 | LOSS: 0.1439 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0517/0938 | LOSS: 0.1439 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0518/0938 | LOSS: 0.1439 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0519/0938 | LOSS: 0.1438 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0520/0938 | LOSS: 0.1437 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0521/0938 | LOSS: 0.1437 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0522/0938 | LOSS: 0.1437 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0523/0938 | LOSS: 0.1436 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0524/0938 | LOSS: 0.1435 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0525/0938 | LOSS: 0.1435 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0526/0938 | LOSS: 0.1434 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0527/0938 | LOSS: 0.1437 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0528/0938 | LOSS: 0.1442 | ACC 0.9581\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0529/0938 | LOSS: 0.1444 | ACC 0.9580\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0530/0938 | LOSS: 0.1447 | ACC 0.9580\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0531/0938 | LOSS: 0.1446 | ACC 0.9580\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0532/0938 | LOSS: 0.1445 | ACC 0.9580\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0533/0938 | LOSS: 0.1443 | ACC 0.9581\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0534/0938 | LOSS: 0.1444 | ACC 0.9581\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0535/0938 | LOSS: 0.1442 | ACC 0.9581\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0536/0938 | LOSS: 0.1442 | ACC 0.9581\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0537/0938 | LOSS: 0.1441 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0538/0938 | LOSS: 0.1441 | ACC 0.9581\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0539/0938 | LOSS: 0.1440 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0540/0938 | LOSS: 0.1438 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0541/0938 | LOSS: 0.1437 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0542/0938 | LOSS: 0.1437 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0543/0938 | LOSS: 0.1436 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0544/0938 | LOSS: 0.1437 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0545/0938 | LOSS: 0.1435 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0546/0938 | LOSS: 0.1435 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0547/0938 | LOSS: 0.1435 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0548/0938 | LOSS: 0.1433 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0549/0938 | LOSS: 0.1433 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0550/0938 | LOSS: 0.1434 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0551/0938 | LOSS: 0.1433 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0552/0938 | LOSS: 0.1434 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0553/0938 | LOSS: 0.1433 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0554/0938 | LOSS: 0.1433 | ACC 0.9582\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0555/0938 | LOSS: 0.1433 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0556/0938 | LOSS: 0.1431 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0557/0938 | LOSS: 0.1432 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0558/0938 | LOSS: 0.1432 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0559/0938 | LOSS: 0.1431 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0560/0938 | LOSS: 0.1430 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0561/0938 | LOSS: 0.1429 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0562/0938 | LOSS: 0.1430 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0563/0938 | LOSS: 0.1429 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0564/0938 | LOSS: 0.1428 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0565/0938 | LOSS: 0.1426 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0566/0938 | LOSS: 0.1428 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0567/0938 | LOSS: 0.1428 | ACC 0.9583\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0568/0938 | LOSS: 0.1426 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0569/0938 | LOSS: 0.1427 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0570/0938 | LOSS: 0.1426 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0571/0938 | LOSS: 0.1426 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0572/0938 | LOSS: 0.1428 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0573/0938 | LOSS: 0.1427 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0574/0938 | LOSS: 0.1426 | ACC 0.9584\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0575/0938 | LOSS: 0.1425 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0576/0938 | LOSS: 0.1424 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0577/0938 | LOSS: 0.1423 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0578/0938 | LOSS: 0.1427 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0579/0938 | LOSS: 0.1427 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0580/0938 | LOSS: 0.1426 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0581/0938 | LOSS: 0.1426 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0582/0938 | LOSS: 0.1424 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0583/0938 | LOSS: 0.1423 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0584/0938 | LOSS: 0.1423 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0585/0938 | LOSS: 0.1423 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0586/0938 | LOSS: 0.1423 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0587/0938 | LOSS: 0.1421 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0588/0938 | LOSS: 0.1421 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0589/0938 | LOSS: 0.1420 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0590/0938 | LOSS: 0.1419 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0591/0938 | LOSS: 0.1420 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0592/0938 | LOSS: 0.1421 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0593/0938 | LOSS: 0.1419 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0594/0938 | LOSS: 0.1419 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0595/0938 | LOSS: 0.1417 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0596/0938 | LOSS: 0.1415 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0597/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0598/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0599/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0600/0938 | LOSS: 0.1413 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0601/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0602/0938 | LOSS: 0.1419 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0603/0938 | LOSS: 0.1417 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0604/0938 | LOSS: 0.1416 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0605/0938 | LOSS: 0.1415 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0606/0938 | LOSS: 0.1415 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0607/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0608/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0609/0938 | LOSS: 0.1416 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0610/0938 | LOSS: 0.1414 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0611/0938 | LOSS: 0.1415 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0612/0938 | LOSS: 0.1413 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0613/0938 | LOSS: 0.1412 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0614/0938 | LOSS: 0.1411 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0615/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0616/0938 | LOSS: 0.1412 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0617/0938 | LOSS: 0.1412 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0618/0938 | LOSS: 0.1410 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0619/0938 | LOSS: 0.1409 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0620/0938 | LOSS: 0.1412 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0621/0938 | LOSS: 0.1412 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0622/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0623/0938 | LOSS: 0.1412 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0624/0938 | LOSS: 0.1411 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0625/0938 | LOSS: 0.1409 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0626/0938 | LOSS: 0.1409 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0627/0938 | LOSS: 0.1409 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0628/0938 | LOSS: 0.1412 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0629/0938 | LOSS: 0.1413 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0630/0938 | LOSS: 0.1419 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0631/0938 | LOSS: 0.1420 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0632/0938 | LOSS: 0.1421 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0633/0938 | LOSS: 0.1420 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0634/0938 | LOSS: 0.1421 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0635/0938 | LOSS: 0.1420 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0636/0938 | LOSS: 0.1421 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0637/0938 | LOSS: 0.1419 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0638/0938 | LOSS: 0.1418 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0639/0938 | LOSS: 0.1421 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0640/0938 | LOSS: 0.1420 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0641/0938 | LOSS: 0.1419 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0642/0938 | LOSS: 0.1420 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0643/0938 | LOSS: 0.1419 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0644/0938 | LOSS: 0.1420 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0645/0938 | LOSS: 0.1423 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0646/0938 | LOSS: 0.1421 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0647/0938 | LOSS: 0.1421 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0648/0938 | LOSS: 0.1420 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0649/0938 | LOSS: 0.1418 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0650/0938 | LOSS: 0.1419 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0651/0938 | LOSS: 0.1419 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0652/0938 | LOSS: 0.1422 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0653/0938 | LOSS: 0.1421 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0654/0938 | LOSS: 0.1421 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0655/0938 | LOSS: 0.1421 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0656/0938 | LOSS: 0.1422 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0657/0938 | LOSS: 0.1423 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0658/0938 | LOSS: 0.1422 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0659/0938 | LOSS: 0.1421 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0660/0938 | LOSS: 0.1419 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0661/0938 | LOSS: 0.1421 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0662/0938 | LOSS: 0.1419 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0663/0938 | LOSS: 0.1419 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0664/0938 | LOSS: 0.1421 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0665/0938 | LOSS: 0.1421 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0666/0938 | LOSS: 0.1420 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0667/0938 | LOSS: 0.1419 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0668/0938 | LOSS: 0.1418 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0669/0938 | LOSS: 0.1417 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0670/0938 | LOSS: 0.1418 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0671/0938 | LOSS: 0.1420 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0672/0938 | LOSS: 0.1418 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0673/0938 | LOSS: 0.1417 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0674/0938 | LOSS: 0.1416 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0675/0938 | LOSS: 0.1415 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0676/0938 | LOSS: 0.1416 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0677/0938 | LOSS: 0.1415 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0678/0938 | LOSS: 0.1415 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0679/0938 | LOSS: 0.1417 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0680/0938 | LOSS: 0.1418 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0681/0938 | LOSS: 0.1416 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0682/0938 | LOSS: 0.1417 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0683/0938 | LOSS: 0.1416 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0684/0938 | LOSS: 0.1417 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0685/0938 | LOSS: 0.1417 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0686/0938 | LOSS: 0.1416 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0687/0938 | LOSS: 0.1416 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0688/0938 | LOSS: 0.1417 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0689/0938 | LOSS: 0.1418 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0690/0938 | LOSS: 0.1417 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0691/0938 | LOSS: 0.1420 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0692/0938 | LOSS: 0.1419 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0693/0938 | LOSS: 0.1421 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0694/0938 | LOSS: 0.1422 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0695/0938 | LOSS: 0.1423 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0696/0938 | LOSS: 0.1421 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0697/0938 | LOSS: 0.1422 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0698/0938 | LOSS: 0.1422 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0699/0938 | LOSS: 0.1423 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0700/0938 | LOSS: 0.1422 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0701/0938 | LOSS: 0.1424 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0702/0938 | LOSS: 0.1424 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0703/0938 | LOSS: 0.1424 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0704/0938 | LOSS: 0.1424 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0705/0938 | LOSS: 0.1423 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0706/0938 | LOSS: 0.1422 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0707/0938 | LOSS: 0.1422 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0708/0938 | LOSS: 0.1422 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0709/0938 | LOSS: 0.1421 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0710/0938 | LOSS: 0.1421 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0711/0938 | LOSS: 0.1422 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0712/0938 | LOSS: 0.1422 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0713/0938 | LOSS: 0.1421 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0714/0938 | LOSS: 0.1421 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0715/0938 | LOSS: 0.1422 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0716/0938 | LOSS: 0.1421 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0717/0938 | LOSS: 0.1421 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0718/0938 | LOSS: 0.1424 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0719/0938 | LOSS: 0.1423 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0720/0938 | LOSS: 0.1423 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0721/0938 | LOSS: 0.1422 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0722/0938 | LOSS: 0.1421 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0723/0938 | LOSS: 0.1420 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0724/0938 | LOSS: 0.1421 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0725/0938 | LOSS: 0.1419 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0726/0938 | LOSS: 0.1419 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0727/0938 | LOSS: 0.1421 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0728/0938 | LOSS: 0.1421 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0729/0938 | LOSS: 0.1421 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0730/0938 | LOSS: 0.1421 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0731/0938 | LOSS: 0.1420 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0732/0938 | LOSS: 0.1421 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0733/0938 | LOSS: 0.1420 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0734/0938 | LOSS: 0.1420 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0735/0938 | LOSS: 0.1419 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0736/0938 | LOSS: 0.1419 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0737/0938 | LOSS: 0.1420 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0738/0938 | LOSS: 0.1419 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0739/0938 | LOSS: 0.1419 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0740/0938 | LOSS: 0.1419 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0741/0938 | LOSS: 0.1418 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0742/0938 | LOSS: 0.1417 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0743/0938 | LOSS: 0.1418 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0744/0938 | LOSS: 0.1417 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0745/0938 | LOSS: 0.1416 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0746/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0747/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0748/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0749/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0750/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0751/0938 | LOSS: 0.1412 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0752/0938 | LOSS: 0.1412 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0753/0938 | LOSS: 0.1411 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0754/0938 | LOSS: 0.1412 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0755/0938 | LOSS: 0.1413 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0756/0938 | LOSS: 0.1412 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0757/0938 | LOSS: 0.1413 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0758/0938 | LOSS: 0.1413 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0759/0938 | LOSS: 0.1413 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0760/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0761/0938 | LOSS: 0.1416 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0762/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0763/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0764/0938 | LOSS: 0.1414 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0765/0938 | LOSS: 0.1413 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0766/0938 | LOSS: 0.1413 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0767/0938 | LOSS: 0.1413 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0768/0938 | LOSS: 0.1414 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0769/0938 | LOSS: 0.1413 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0770/0938 | LOSS: 0.1413 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0771/0938 | LOSS: 0.1414 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0772/0938 | LOSS: 0.1415 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0773/0938 | LOSS: 0.1415 | ACC 0.9585\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0774/0938 | LOSS: 0.1415 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0775/0938 | LOSS: 0.1417 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0776/0938 | LOSS: 0.1417 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0777/0938 | LOSS: 0.1416 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0778/0938 | LOSS: 0.1415 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0779/0938 | LOSS: 0.1416 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0780/0938 | LOSS: 0.1414 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0781/0938 | LOSS: 0.1415 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0782/0938 | LOSS: 0.1415 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0783/0938 | LOSS: 0.1414 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0784/0938 | LOSS: 0.1414 | ACC 0.9586\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0785/0938 | LOSS: 0.1413 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0786/0938 | LOSS: 0.1412 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0787/0938 | LOSS: 0.1414 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0788/0938 | LOSS: 0.1413 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0789/0938 | LOSS: 0.1414 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0790/0938 | LOSS: 0.1413 | ACC 0.9587\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0791/0938 | LOSS: 0.1412 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0792/0938 | LOSS: 0.1412 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0793/0938 | LOSS: 0.1412 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0794/0938 | LOSS: 0.1411 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0795/0938 | LOSS: 0.1410 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0796/0938 | LOSS: 0.1410 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0797/0938 | LOSS: 0.1410 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0798/0938 | LOSS: 0.1410 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0799/0938 | LOSS: 0.1411 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0800/0938 | LOSS: 0.1411 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0801/0938 | LOSS: 0.1410 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0802/0938 | LOSS: 0.1410 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0803/0938 | LOSS: 0.1411 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0804/0938 | LOSS: 0.1411 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0805/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0806/0938 | LOSS: 0.1412 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0807/0938 | LOSS: 0.1412 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0808/0938 | LOSS: 0.1412 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0809/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0810/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0811/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0812/0938 | LOSS: 0.1412 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0813/0938 | LOSS: 0.1412 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0814/0938 | LOSS: 0.1412 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0815/0938 | LOSS: 0.1411 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0816/0938 | LOSS: 0.1412 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0817/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0818/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0819/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0820/0938 | LOSS: 0.1415 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0821/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0822/0938 | LOSS: 0.1413 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0823/0938 | LOSS: 0.1412 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0824/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0825/0938 | LOSS: 0.1412 | ACC 0.9588\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0826/0938 | LOSS: 0.1411 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0827/0938 | LOSS: 0.1410 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0828/0938 | LOSS: 0.1409 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0829/0938 | LOSS: 0.1409 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0830/0938 | LOSS: 0.1408 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0831/0938 | LOSS: 0.1407 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0832/0938 | LOSS: 0.1409 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0833/0938 | LOSS: 0.1411 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0834/0938 | LOSS: 0.1410 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0835/0938 | LOSS: 0.1409 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0836/0938 | LOSS: 0.1409 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0837/0938 | LOSS: 0.1409 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0838/0938 | LOSS: 0.1410 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0839/0938 | LOSS: 0.1409 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0840/0938 | LOSS: 0.1410 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0841/0938 | LOSS: 0.1409 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0842/0938 | LOSS: 0.1409 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0843/0938 | LOSS: 0.1408 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0844/0938 | LOSS: 0.1408 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0845/0938 | LOSS: 0.1408 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0846/0938 | LOSS: 0.1407 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0847/0938 | LOSS: 0.1407 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0848/0938 | LOSS: 0.1406 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0849/0938 | LOSS: 0.1406 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0850/0938 | LOSS: 0.1406 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0851/0938 | LOSS: 0.1406 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0852/0938 | LOSS: 0.1406 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0853/0938 | LOSS: 0.1405 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0854/0938 | LOSS: 0.1406 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0855/0938 | LOSS: 0.1405 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0856/0938 | LOSS: 0.1405 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0857/0938 | LOSS: 0.1406 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0858/0938 | LOSS: 0.1405 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0859/0938 | LOSS: 0.1404 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0860/0938 | LOSS: 0.1404 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0861/0938 | LOSS: 0.1403 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0862/0938 | LOSS: 0.1404 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0863/0938 | LOSS: 0.1405 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0864/0938 | LOSS: 0.1405 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0865/0938 | LOSS: 0.1405 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0866/0938 | LOSS: 0.1406 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0867/0938 | LOSS: 0.1405 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0868/0938 | LOSS: 0.1406 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0869/0938 | LOSS: 0.1406 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0870/0938 | LOSS: 0.1406 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0871/0938 | LOSS: 0.1406 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0872/0938 | LOSS: 0.1405 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0873/0938 | LOSS: 0.1406 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0874/0938 | LOSS: 0.1406 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0875/0938 | LOSS: 0.1407 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0876/0938 | LOSS: 0.1409 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0877/0938 | LOSS: 0.1408 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0878/0938 | LOSS: 0.1408 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0879/0938 | LOSS: 0.1407 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0880/0938 | LOSS: 0.1408 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0881/0938 | LOSS: 0.1409 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0882/0938 | LOSS: 0.1409 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0883/0938 | LOSS: 0.1408 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0884/0938 | LOSS: 0.1408 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0885/0938 | LOSS: 0.1411 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0886/0938 | LOSS: 0.1413 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0887/0938 | LOSS: 0.1412 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0888/0938 | LOSS: 0.1412 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0889/0938 | LOSS: 0.1411 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0890/0938 | LOSS: 0.1411 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0891/0938 | LOSS: 0.1410 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0892/0938 | LOSS: 0.1410 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0893/0938 | LOSS: 0.1410 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0894/0938 | LOSS: 0.1410 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0895/0938 | LOSS: 0.1409 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0896/0938 | LOSS: 0.1408 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0897/0938 | LOSS: 0.1407 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0898/0938 | LOSS: 0.1407 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0899/0938 | LOSS: 0.1407 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0900/0938 | LOSS: 0.1406 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0901/0938 | LOSS: 0.1406 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0902/0938 | LOSS: 0.1406 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0903/0938 | LOSS: 0.1406 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0904/0938 | LOSS: 0.1408 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0905/0938 | LOSS: 0.1407 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0906/0938 | LOSS: 0.1407 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0907/0938 | LOSS: 0.1406 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0908/0938 | LOSS: 0.1407 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0909/0938 | LOSS: 0.1409 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0910/0938 | LOSS: 0.1408 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0911/0938 | LOSS: 0.1409 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0912/0938 | LOSS: 0.1408 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0913/0938 | LOSS: 0.1408 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0914/0938 | LOSS: 0.1407 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0915/0938 | LOSS: 0.1406 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0916/0938 | LOSS: 0.1407 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0917/0938 | LOSS: 0.1405 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0918/0938 | LOSS: 0.1405 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0919/0938 | LOSS: 0.1406 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0920/0938 | LOSS: 0.1406 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0921/0938 | LOSS: 0.1407 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0922/0938 | LOSS: 0.1407 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0923/0938 | LOSS: 0.1407 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0924/0938 | LOSS: 0.1406 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0925/0938 | LOSS: 0.1406 | ACC 0.9593\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0926/0938 | LOSS: 0.1407 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0927/0938 | LOSS: 0.1407 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0928/0938 | LOSS: 0.1407 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0929/0938 | LOSS: 0.1407 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0930/0938 | LOSS: 0.1408 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0931/0938 | LOSS: 0.1408 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0932/0938 | LOSS: 0.1408 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0933/0938 | LOSS: 0.1408 | ACC 0.9592\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0934/0938 | LOSS: 0.1410 | ACC 0.9591\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0935/0938 | LOSS: 0.1411 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0936/0938 | LOSS: 0.1413 | ACC 0.9590\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0937/0938 | LOSS: 0.1415 | ACC 0.9589\n",
            "TRAIN: EPOCH 0009/0010 | BATCH 0938/0938 | LOSS: 0.1414 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0001/0938 | LOSS: 0.2064 | ACC 0.9531\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0002/0938 | LOSS: 0.1722 | ACC 0.9609\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0003/0938 | LOSS: 0.1358 | ACC 0.9688\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0004/0938 | LOSS: 0.1344 | ACC 0.9648\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0005/0938 | LOSS: 0.1474 | ACC 0.9563\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0006/0938 | LOSS: 0.1445 | ACC 0.9609\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0007/0938 | LOSS: 0.1391 | ACC 0.9598\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0008/0938 | LOSS: 0.1251 | ACC 0.9648\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0009/0938 | LOSS: 0.1272 | ACC 0.9635\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0010/0938 | LOSS: 0.1225 | ACC 0.9656\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0011/0938 | LOSS: 0.1272 | ACC 0.9645\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0012/0938 | LOSS: 0.1245 | ACC 0.9661\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0013/0938 | LOSS: 0.1210 | ACC 0.9663\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0014/0938 | LOSS: 0.1186 | ACC 0.9665\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0015/0938 | LOSS: 0.1205 | ACC 0.9656\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0016/0938 | LOSS: 0.1218 | ACC 0.9639\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0017/0938 | LOSS: 0.1170 | ACC 0.9651\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0018/0938 | LOSS: 0.1223 | ACC 0.9627\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0019/0938 | LOSS: 0.1236 | ACC 0.9630\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0020/0938 | LOSS: 0.1194 | ACC 0.9641\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0021/0938 | LOSS: 0.1223 | ACC 0.9621\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0022/0938 | LOSS: 0.1253 | ACC 0.9616\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0023/0938 | LOSS: 0.1249 | ACC 0.9606\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0024/0938 | LOSS: 0.1253 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0025/0938 | LOSS: 0.1251 | ACC 0.9606\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0026/0938 | LOSS: 0.1349 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0027/0938 | LOSS: 0.1360 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0028/0938 | LOSS: 0.1371 | ACC 0.9581\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0029/0938 | LOSS: 0.1383 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0030/0938 | LOSS: 0.1402 | ACC 0.9578\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0031/0938 | LOSS: 0.1397 | ACC 0.9577\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0032/0938 | LOSS: 0.1388 | ACC 0.9575\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0033/0938 | LOSS: 0.1396 | ACC 0.9574\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0034/0938 | LOSS: 0.1371 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0035/0938 | LOSS: 0.1363 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0036/0938 | LOSS: 0.1357 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0037/0938 | LOSS: 0.1362 | ACC 0.9578\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0038/0938 | LOSS: 0.1389 | ACC 0.9572\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0039/0938 | LOSS: 0.1530 | ACC 0.9571\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0040/0938 | LOSS: 0.1534 | ACC 0.9566\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0041/0938 | LOSS: 0.1523 | ACC 0.9569\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0042/0938 | LOSS: 0.1505 | ACC 0.9572\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0043/0938 | LOSS: 0.1527 | ACC 0.9571\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0044/0938 | LOSS: 0.1502 | ACC 0.9577\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0045/0938 | LOSS: 0.1494 | ACC 0.9580\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0046/0938 | LOSS: 0.1487 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0047/0938 | LOSS: 0.1480 | ACC 0.9578\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0048/0938 | LOSS: 0.1462 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0049/0938 | LOSS: 0.1449 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0050/0938 | LOSS: 0.1442 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0051/0938 | LOSS: 0.1447 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0052/0938 | LOSS: 0.1429 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0053/0938 | LOSS: 0.1415 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0054/0938 | LOSS: 0.1443 | ACC 0.9598\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0055/0938 | LOSS: 0.1449 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0056/0938 | LOSS: 0.1441 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0057/0938 | LOSS: 0.1436 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0058/0938 | LOSS: 0.1439 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0059/0938 | LOSS: 0.1434 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0060/0938 | LOSS: 0.1436 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0061/0938 | LOSS: 0.1425 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0062/0938 | LOSS: 0.1415 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0063/0938 | LOSS: 0.1405 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0064/0938 | LOSS: 0.1394 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0065/0938 | LOSS: 0.1392 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0066/0938 | LOSS: 0.1382 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0067/0938 | LOSS: 0.1372 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0068/0938 | LOSS: 0.1377 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0069/0938 | LOSS: 0.1373 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0070/0938 | LOSS: 0.1387 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0071/0938 | LOSS: 0.1376 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0072/0938 | LOSS: 0.1369 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0073/0938 | LOSS: 0.1362 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0074/0938 | LOSS: 0.1358 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0075/0938 | LOSS: 0.1365 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0076/0938 | LOSS: 0.1378 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0077/0938 | LOSS: 0.1368 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0078/0938 | LOSS: 0.1360 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0079/0938 | LOSS: 0.1356 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0080/0938 | LOSS: 0.1356 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0081/0938 | LOSS: 0.1360 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0082/0938 | LOSS: 0.1355 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0083/0938 | LOSS: 0.1357 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0084/0938 | LOSS: 0.1368 | ACC 0.9581\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0085/0938 | LOSS: 0.1360 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0086/0938 | LOSS: 0.1380 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0087/0938 | LOSS: 0.1386 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0088/0938 | LOSS: 0.1382 | ACC 0.9581\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0089/0938 | LOSS: 0.1375 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0090/0938 | LOSS: 0.1374 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0091/0938 | LOSS: 0.1388 | ACC 0.9578\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0092/0938 | LOSS: 0.1395 | ACC 0.9577\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0093/0938 | LOSS: 0.1399 | ACC 0.9572\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0094/0938 | LOSS: 0.1387 | ACC 0.9576\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0095/0938 | LOSS: 0.1382 | ACC 0.9576\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0096/0938 | LOSS: 0.1384 | ACC 0.9578\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0097/0938 | LOSS: 0.1377 | ACC 0.9581\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0098/0938 | LOSS: 0.1373 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0099/0938 | LOSS: 0.1363 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0100/0938 | LOSS: 0.1366 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0101/0938 | LOSS: 0.1360 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0102/0938 | LOSS: 0.1358 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0103/0938 | LOSS: 0.1366 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0104/0938 | LOSS: 0.1369 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0105/0938 | LOSS: 0.1364 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0106/0938 | LOSS: 0.1374 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0107/0938 | LOSS: 0.1378 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0108/0938 | LOSS: 0.1372 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0109/0938 | LOSS: 0.1375 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0110/0938 | LOSS: 0.1376 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0111/0938 | LOSS: 0.1372 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0112/0938 | LOSS: 0.1366 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0113/0938 | LOSS: 0.1372 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0114/0938 | LOSS: 0.1361 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0115/0938 | LOSS: 0.1354 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0116/0938 | LOSS: 0.1348 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0117/0938 | LOSS: 0.1345 | ACC 0.9598\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0118/0938 | LOSS: 0.1348 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0119/0938 | LOSS: 0.1351 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0120/0938 | LOSS: 0.1349 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0121/0938 | LOSS: 0.1365 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0122/0938 | LOSS: 0.1365 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0123/0938 | LOSS: 0.1369 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0124/0938 | LOSS: 0.1366 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0125/0938 | LOSS: 0.1364 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0126/0938 | LOSS: 0.1377 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0127/0938 | LOSS: 0.1376 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0128/0938 | LOSS: 0.1378 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0129/0938 | LOSS: 0.1371 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0130/0938 | LOSS: 0.1384 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0131/0938 | LOSS: 0.1389 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0132/0938 | LOSS: 0.1380 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0133/0938 | LOSS: 0.1388 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0134/0938 | LOSS: 0.1391 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0135/0938 | LOSS: 0.1394 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0136/0938 | LOSS: 0.1386 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0137/0938 | LOSS: 0.1384 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0138/0938 | LOSS: 0.1390 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0139/0938 | LOSS: 0.1384 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0140/0938 | LOSS: 0.1377 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0141/0938 | LOSS: 0.1383 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0142/0938 | LOSS: 0.1381 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0143/0938 | LOSS: 0.1375 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0144/0938 | LOSS: 0.1384 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0145/0938 | LOSS: 0.1382 | ACC 0.9598\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0146/0938 | LOSS: 0.1382 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0147/0938 | LOSS: 0.1376 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0148/0938 | LOSS: 0.1372 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0149/0938 | LOSS: 0.1370 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0150/0938 | LOSS: 0.1362 | ACC 0.9604\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0151/0938 | LOSS: 0.1362 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0152/0938 | LOSS: 0.1355 | ACC 0.9605\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0153/0938 | LOSS: 0.1359 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0154/0938 | LOSS: 0.1361 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0155/0938 | LOSS: 0.1360 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0156/0938 | LOSS: 0.1362 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0157/0938 | LOSS: 0.1356 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0158/0938 | LOSS: 0.1358 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0159/0938 | LOSS: 0.1354 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0160/0938 | LOSS: 0.1355 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0161/0938 | LOSS: 0.1351 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0162/0938 | LOSS: 0.1357 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0163/0938 | LOSS: 0.1350 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0164/0938 | LOSS: 0.1344 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0165/0938 | LOSS: 0.1343 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0166/0938 | LOSS: 0.1339 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0167/0938 | LOSS: 0.1342 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0168/0938 | LOSS: 0.1339 | ACC 0.9604\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0169/0938 | LOSS: 0.1338 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0170/0938 | LOSS: 0.1338 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0171/0938 | LOSS: 0.1338 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0172/0938 | LOSS: 0.1335 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0173/0938 | LOSS: 0.1331 | ACC 0.9604\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0174/0938 | LOSS: 0.1329 | ACC 0.9605\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0175/0938 | LOSS: 0.1324 | ACC 0.9607\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0176/0938 | LOSS: 0.1320 | ACC 0.9608\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0177/0938 | LOSS: 0.1315 | ACC 0.9610\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0178/0938 | LOSS: 0.1318 | ACC 0.9609\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0179/0938 | LOSS: 0.1320 | ACC 0.9608\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0180/0938 | LOSS: 0.1315 | ACC 0.9609\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0181/0938 | LOSS: 0.1309 | ACC 0.9612\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0182/0938 | LOSS: 0.1319 | ACC 0.9609\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0183/0938 | LOSS: 0.1315 | ACC 0.9611\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0184/0938 | LOSS: 0.1312 | ACC 0.9612\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0185/0938 | LOSS: 0.1307 | ACC 0.9614\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0186/0938 | LOSS: 0.1305 | ACC 0.9614\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0187/0938 | LOSS: 0.1304 | ACC 0.9616\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0188/0938 | LOSS: 0.1305 | ACC 0.9614\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0189/0938 | LOSS: 0.1312 | ACC 0.9613\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0190/0938 | LOSS: 0.1306 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0191/0938 | LOSS: 0.1309 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0192/0938 | LOSS: 0.1308 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0193/0938 | LOSS: 0.1304 | ACC 0.9616\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0194/0938 | LOSS: 0.1311 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0195/0938 | LOSS: 0.1309 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0196/0938 | LOSS: 0.1309 | ACC 0.9613\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0197/0938 | LOSS: 0.1318 | ACC 0.9612\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0198/0938 | LOSS: 0.1315 | ACC 0.9613\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0199/0938 | LOSS: 0.1319 | ACC 0.9611\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0200/0938 | LOSS: 0.1319 | ACC 0.9610\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0201/0938 | LOSS: 0.1323 | ACC 0.9608\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0202/0938 | LOSS: 0.1319 | ACC 0.9609\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0203/0938 | LOSS: 0.1319 | ACC 0.9610\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0204/0938 | LOSS: 0.1317 | ACC 0.9611\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0205/0938 | LOSS: 0.1317 | ACC 0.9611\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0206/0938 | LOSS: 0.1318 | ACC 0.9611\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0207/0938 | LOSS: 0.1317 | ACC 0.9611\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0208/0938 | LOSS: 0.1320 | ACC 0.9609\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0209/0938 | LOSS: 0.1318 | ACC 0.9610\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0210/0938 | LOSS: 0.1315 | ACC 0.9612\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0211/0938 | LOSS: 0.1310 | ACC 0.9613\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0212/0938 | LOSS: 0.1308 | ACC 0.9613\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0213/0938 | LOSS: 0.1304 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0214/0938 | LOSS: 0.1301 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0215/0938 | LOSS: 0.1300 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0216/0938 | LOSS: 0.1303 | ACC 0.9614\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0217/0938 | LOSS: 0.1301 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0218/0938 | LOSS: 0.1298 | ACC 0.9616\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0219/0938 | LOSS: 0.1296 | ACC 0.9616\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0220/0938 | LOSS: 0.1294 | ACC 0.9616\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0221/0938 | LOSS: 0.1296 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0222/0938 | LOSS: 0.1304 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0223/0938 | LOSS: 0.1305 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0224/0938 | LOSS: 0.1305 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0225/0938 | LOSS: 0.1306 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0226/0938 | LOSS: 0.1306 | ACC 0.9614\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0227/0938 | LOSS: 0.1311 | ACC 0.9613\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0228/0938 | LOSS: 0.1308 | ACC 0.9614\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0229/0938 | LOSS: 0.1304 | ACC 0.9616\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0230/0938 | LOSS: 0.1305 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0231/0938 | LOSS: 0.1306 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0232/0938 | LOSS: 0.1310 | ACC 0.9614\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0233/0938 | LOSS: 0.1309 | ACC 0.9614\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0234/0938 | LOSS: 0.1305 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0235/0938 | LOSS: 0.1316 | ACC 0.9614\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0236/0938 | LOSS: 0.1311 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0237/0938 | LOSS: 0.1307 | ACC 0.9617\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0238/0938 | LOSS: 0.1308 | ACC 0.9617\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0239/0938 | LOSS: 0.1305 | ACC 0.9618\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0240/0938 | LOSS: 0.1302 | ACC 0.9618\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0241/0938 | LOSS: 0.1302 | ACC 0.9617\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0242/0938 | LOSS: 0.1311 | ACC 0.9616\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0243/0938 | LOSS: 0.1308 | ACC 0.9618\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0244/0938 | LOSS: 0.1311 | ACC 0.9617\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0245/0938 | LOSS: 0.1315 | ACC 0.9616\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0246/0938 | LOSS: 0.1317 | ACC 0.9615\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0247/0938 | LOSS: 0.1319 | ACC 0.9613\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0248/0938 | LOSS: 0.1318 | ACC 0.9614\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0249/0938 | LOSS: 0.1322 | ACC 0.9612\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0250/0938 | LOSS: 0.1321 | ACC 0.9611\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0251/0938 | LOSS: 0.1323 | ACC 0.9612\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0252/0938 | LOSS: 0.1320 | ACC 0.9612\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0253/0938 | LOSS: 0.1316 | ACC 0.9613\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0254/0938 | LOSS: 0.1320 | ACC 0.9613\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0255/0938 | LOSS: 0.1328 | ACC 0.9612\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0256/0938 | LOSS: 0.1328 | ACC 0.9611\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0257/0938 | LOSS: 0.1329 | ACC 0.9611\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0258/0938 | LOSS: 0.1327 | ACC 0.9611\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0259/0938 | LOSS: 0.1328 | ACC 0.9608\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0260/0938 | LOSS: 0.1329 | ACC 0.9608\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0261/0938 | LOSS: 0.1331 | ACC 0.9607\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0262/0938 | LOSS: 0.1333 | ACC 0.9606\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0263/0938 | LOSS: 0.1335 | ACC 0.9606\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0264/0938 | LOSS: 0.1333 | ACC 0.9607\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0265/0938 | LOSS: 0.1333 | ACC 0.9607\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0266/0938 | LOSS: 0.1332 | ACC 0.9608\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0267/0938 | LOSS: 0.1329 | ACC 0.9608\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0268/0938 | LOSS: 0.1330 | ACC 0.9608\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0269/0938 | LOSS: 0.1330 | ACC 0.9607\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0270/0938 | LOSS: 0.1335 | ACC 0.9605\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0271/0938 | LOSS: 0.1338 | ACC 0.9604\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0272/0938 | LOSS: 0.1341 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0273/0938 | LOSS: 0.1342 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0274/0938 | LOSS: 0.1340 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0275/0938 | LOSS: 0.1337 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0276/0938 | LOSS: 0.1342 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0277/0938 | LOSS: 0.1343 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0278/0938 | LOSS: 0.1346 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0279/0938 | LOSS: 0.1345 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0280/0938 | LOSS: 0.1345 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0281/0938 | LOSS: 0.1346 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0282/0938 | LOSS: 0.1347 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0283/0938 | LOSS: 0.1350 | ACC 0.9598\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0284/0938 | LOSS: 0.1346 | ACC 0.9598\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0285/0938 | LOSS: 0.1343 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0286/0938 | LOSS: 0.1339 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0287/0938 | LOSS: 0.1337 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0288/0938 | LOSS: 0.1334 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0289/0938 | LOSS: 0.1337 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0290/0938 | LOSS: 0.1334 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0291/0938 | LOSS: 0.1333 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0292/0938 | LOSS: 0.1330 | ACC 0.9605\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0293/0938 | LOSS: 0.1329 | ACC 0.9605\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0294/0938 | LOSS: 0.1331 | ACC 0.9605\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0295/0938 | LOSS: 0.1329 | ACC 0.9605\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0296/0938 | LOSS: 0.1329 | ACC 0.9605\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0297/0938 | LOSS: 0.1329 | ACC 0.9604\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0298/0938 | LOSS: 0.1332 | ACC 0.9604\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0299/0938 | LOSS: 0.1334 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0300/0938 | LOSS: 0.1338 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0301/0938 | LOSS: 0.1341 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0302/0938 | LOSS: 0.1338 | ACC 0.9604\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0303/0938 | LOSS: 0.1337 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0304/0938 | LOSS: 0.1340 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0305/0938 | LOSS: 0.1337 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0306/0938 | LOSS: 0.1338 | ACC 0.9604\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0307/0938 | LOSS: 0.1334 | ACC 0.9605\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0308/0938 | LOSS: 0.1332 | ACC 0.9606\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0309/0938 | LOSS: 0.1333 | ACC 0.9606\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0310/0938 | LOSS: 0.1336 | ACC 0.9605\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0311/0938 | LOSS: 0.1333 | ACC 0.9606\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0312/0938 | LOSS: 0.1335 | ACC 0.9605\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0313/0938 | LOSS: 0.1334 | ACC 0.9606\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0314/0938 | LOSS: 0.1334 | ACC 0.9604\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0315/0938 | LOSS: 0.1340 | ACC 0.9604\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0316/0938 | LOSS: 0.1344 | ACC 0.9604\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0317/0938 | LOSS: 0.1344 | ACC 0.9604\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0318/0938 | LOSS: 0.1350 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0319/0938 | LOSS: 0.1357 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0320/0938 | LOSS: 0.1359 | ACC 0.9603\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0321/0938 | LOSS: 0.1362 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0322/0938 | LOSS: 0.1361 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0323/0938 | LOSS: 0.1361 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0324/0938 | LOSS: 0.1362 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0325/0938 | LOSS: 0.1364 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0326/0938 | LOSS: 0.1366 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0327/0938 | LOSS: 0.1366 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0328/0938 | LOSS: 0.1365 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0329/0938 | LOSS: 0.1365 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0330/0938 | LOSS: 0.1364 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0331/0938 | LOSS: 0.1364 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0332/0938 | LOSS: 0.1363 | ACC 0.9598\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0333/0938 | LOSS: 0.1363 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0334/0938 | LOSS: 0.1364 | ACC 0.9598\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0335/0938 | LOSS: 0.1363 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0336/0938 | LOSS: 0.1365 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0337/0938 | LOSS: 0.1363 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0338/0938 | LOSS: 0.1364 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0339/0938 | LOSS: 0.1366 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0340/0938 | LOSS: 0.1365 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0341/0938 | LOSS: 0.1365 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0342/0938 | LOSS: 0.1364 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0343/0938 | LOSS: 0.1367 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0344/0938 | LOSS: 0.1369 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0345/0938 | LOSS: 0.1368 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0346/0938 | LOSS: 0.1367 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0347/0938 | LOSS: 0.1368 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0348/0938 | LOSS: 0.1369 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0349/0938 | LOSS: 0.1372 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0350/0938 | LOSS: 0.1373 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0351/0938 | LOSS: 0.1377 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0352/0938 | LOSS: 0.1378 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0353/0938 | LOSS: 0.1376 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0354/0938 | LOSS: 0.1377 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0355/0938 | LOSS: 0.1376 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0356/0938 | LOSS: 0.1374 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0357/0938 | LOSS: 0.1374 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0358/0938 | LOSS: 0.1372 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0359/0938 | LOSS: 0.1370 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0360/0938 | LOSS: 0.1369 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0361/0938 | LOSS: 0.1368 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0362/0938 | LOSS: 0.1365 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0363/0938 | LOSS: 0.1371 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0364/0938 | LOSS: 0.1369 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0365/0938 | LOSS: 0.1373 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0366/0938 | LOSS: 0.1374 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0367/0938 | LOSS: 0.1376 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0368/0938 | LOSS: 0.1374 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0369/0938 | LOSS: 0.1371 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0370/0938 | LOSS: 0.1370 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0371/0938 | LOSS: 0.1369 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0372/0938 | LOSS: 0.1368 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0373/0938 | LOSS: 0.1369 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0374/0938 | LOSS: 0.1373 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0375/0938 | LOSS: 0.1374 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0376/0938 | LOSS: 0.1373 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0377/0938 | LOSS: 0.1378 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0378/0938 | LOSS: 0.1379 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0379/0938 | LOSS: 0.1378 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0380/0938 | LOSS: 0.1376 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0381/0938 | LOSS: 0.1378 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0382/0938 | LOSS: 0.1376 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0383/0938 | LOSS: 0.1376 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0384/0938 | LOSS: 0.1376 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0385/0938 | LOSS: 0.1374 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0386/0938 | LOSS: 0.1379 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0387/0938 | LOSS: 0.1376 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0388/0938 | LOSS: 0.1377 | ACC 0.9590\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0389/0938 | LOSS: 0.1375 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0390/0938 | LOSS: 0.1378 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0391/0938 | LOSS: 0.1376 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0392/0938 | LOSS: 0.1377 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0393/0938 | LOSS: 0.1378 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0394/0938 | LOSS: 0.1376 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0395/0938 | LOSS: 0.1374 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0396/0938 | LOSS: 0.1373 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0397/0938 | LOSS: 0.1374 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0398/0938 | LOSS: 0.1373 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0399/0938 | LOSS: 0.1372 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0400/0938 | LOSS: 0.1370 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0401/0938 | LOSS: 0.1371 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0402/0938 | LOSS: 0.1374 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0403/0938 | LOSS: 0.1374 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0404/0938 | LOSS: 0.1374 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0405/0938 | LOSS: 0.1374 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0406/0938 | LOSS: 0.1374 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0407/0938 | LOSS: 0.1377 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0408/0938 | LOSS: 0.1375 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0409/0938 | LOSS: 0.1375 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0410/0938 | LOSS: 0.1375 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0411/0938 | LOSS: 0.1375 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0412/0938 | LOSS: 0.1373 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0413/0938 | LOSS: 0.1373 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0414/0938 | LOSS: 0.1375 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0415/0938 | LOSS: 0.1373 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0416/0938 | LOSS: 0.1373 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0417/0938 | LOSS: 0.1371 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0418/0938 | LOSS: 0.1370 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0419/0938 | LOSS: 0.1370 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0420/0938 | LOSS: 0.1370 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0421/0938 | LOSS: 0.1372 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0422/0938 | LOSS: 0.1371 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0423/0938 | LOSS: 0.1371 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0424/0938 | LOSS: 0.1369 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0425/0938 | LOSS: 0.1367 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0426/0938 | LOSS: 0.1366 | ACC 0.9598\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0427/0938 | LOSS: 0.1368 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0428/0938 | LOSS: 0.1367 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0429/0938 | LOSS: 0.1367 | ACC 0.9598\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0430/0938 | LOSS: 0.1365 | ACC 0.9598\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0431/0938 | LOSS: 0.1369 | ACC 0.9598\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0432/0938 | LOSS: 0.1370 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0433/0938 | LOSS: 0.1368 | ACC 0.9598\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0434/0938 | LOSS: 0.1368 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0435/0938 | LOSS: 0.1366 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0436/0938 | LOSS: 0.1367 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0437/0938 | LOSS: 0.1365 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0438/0938 | LOSS: 0.1363 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0439/0938 | LOSS: 0.1365 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0440/0938 | LOSS: 0.1365 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0441/0938 | LOSS: 0.1365 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0442/0938 | LOSS: 0.1365 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0443/0938 | LOSS: 0.1364 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0444/0938 | LOSS: 0.1363 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0445/0938 | LOSS: 0.1361 | ACC 0.9601\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0446/0938 | LOSS: 0.1359 | ACC 0.9602\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0447/0938 | LOSS: 0.1365 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0448/0938 | LOSS: 0.1365 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0449/0938 | LOSS: 0.1363 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0450/0938 | LOSS: 0.1364 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0451/0938 | LOSS: 0.1364 | ACC 0.9600\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0452/0938 | LOSS: 0.1363 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0453/0938 | LOSS: 0.1366 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0454/0938 | LOSS: 0.1365 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0455/0938 | LOSS: 0.1370 | ACC 0.9599\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0456/0938 | LOSS: 0.1375 | ACC 0.9598\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0457/0938 | LOSS: 0.1376 | ACC 0.9598\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0458/0938 | LOSS: 0.1375 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0459/0938 | LOSS: 0.1381 | ACC 0.9595\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0460/0938 | LOSS: 0.1379 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0461/0938 | LOSS: 0.1382 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0462/0938 | LOSS: 0.1383 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0463/0938 | LOSS: 0.1384 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0464/0938 | LOSS: 0.1383 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0465/0938 | LOSS: 0.1383 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0466/0938 | LOSS: 0.1383 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0467/0938 | LOSS: 0.1381 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0468/0938 | LOSS: 0.1380 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0469/0938 | LOSS: 0.1379 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0470/0938 | LOSS: 0.1378 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0471/0938 | LOSS: 0.1380 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0472/0938 | LOSS: 0.1381 | ACC 0.9597\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0473/0938 | LOSS: 0.1385 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0474/0938 | LOSS: 0.1388 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0475/0938 | LOSS: 0.1386 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0476/0938 | LOSS: 0.1385 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0477/0938 | LOSS: 0.1385 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0478/0938 | LOSS: 0.1392 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0479/0938 | LOSS: 0.1390 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0480/0938 | LOSS: 0.1391 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0481/0938 | LOSS: 0.1390 | ACC 0.9596\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0482/0938 | LOSS: 0.1394 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0483/0938 | LOSS: 0.1396 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0484/0938 | LOSS: 0.1397 | ACC 0.9594\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0485/0938 | LOSS: 0.1401 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0486/0938 | LOSS: 0.1401 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0487/0938 | LOSS: 0.1401 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0488/0938 | LOSS: 0.1399 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0489/0938 | LOSS: 0.1400 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0490/0938 | LOSS: 0.1404 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0491/0938 | LOSS: 0.1406 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0492/0938 | LOSS: 0.1407 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0493/0938 | LOSS: 0.1407 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0494/0938 | LOSS: 0.1407 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0495/0938 | LOSS: 0.1405 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0496/0938 | LOSS: 0.1405 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0497/0938 | LOSS: 0.1406 | ACC 0.9593\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0498/0938 | LOSS: 0.1407 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0499/0938 | LOSS: 0.1406 | ACC 0.9592\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0500/0938 | LOSS: 0.1408 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0501/0938 | LOSS: 0.1409 | ACC 0.9591\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0502/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0503/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0504/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0505/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0506/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0507/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0508/0938 | LOSS: 0.1415 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0509/0938 | LOSS: 0.1416 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0510/0938 | LOSS: 0.1418 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0511/0938 | LOSS: 0.1416 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0512/0938 | LOSS: 0.1416 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0513/0938 | LOSS: 0.1415 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0514/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0515/0938 | LOSS: 0.1415 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0516/0938 | LOSS: 0.1415 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0517/0938 | LOSS: 0.1415 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0518/0938 | LOSS: 0.1415 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0519/0938 | LOSS: 0.1414 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0520/0938 | LOSS: 0.1416 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0521/0938 | LOSS: 0.1419 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0522/0938 | LOSS: 0.1417 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0523/0938 | LOSS: 0.1417 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0524/0938 | LOSS: 0.1416 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0525/0938 | LOSS: 0.1415 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0526/0938 | LOSS: 0.1415 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0527/0938 | LOSS: 0.1413 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0528/0938 | LOSS: 0.1412 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0529/0938 | LOSS: 0.1411 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0530/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0531/0938 | LOSS: 0.1414 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0532/0938 | LOSS: 0.1413 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0533/0938 | LOSS: 0.1411 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0534/0938 | LOSS: 0.1411 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0535/0938 | LOSS: 0.1415 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0536/0938 | LOSS: 0.1415 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0537/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0538/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0539/0938 | LOSS: 0.1414 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0540/0938 | LOSS: 0.1415 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0541/0938 | LOSS: 0.1416 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0542/0938 | LOSS: 0.1416 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0543/0938 | LOSS: 0.1417 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0544/0938 | LOSS: 0.1420 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0545/0938 | LOSS: 0.1421 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0546/0938 | LOSS: 0.1420 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0547/0938 | LOSS: 0.1419 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0548/0938 | LOSS: 0.1420 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0549/0938 | LOSS: 0.1420 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0550/0938 | LOSS: 0.1418 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0551/0938 | LOSS: 0.1418 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0552/0938 | LOSS: 0.1417 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0553/0938 | LOSS: 0.1416 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0554/0938 | LOSS: 0.1415 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0555/0938 | LOSS: 0.1415 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0556/0938 | LOSS: 0.1414 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0557/0938 | LOSS: 0.1416 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0558/0938 | LOSS: 0.1414 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0559/0938 | LOSS: 0.1415 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0560/0938 | LOSS: 0.1417 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0561/0938 | LOSS: 0.1417 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0562/0938 | LOSS: 0.1416 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0563/0938 | LOSS: 0.1418 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0564/0938 | LOSS: 0.1417 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0565/0938 | LOSS: 0.1417 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0566/0938 | LOSS: 0.1415 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0567/0938 | LOSS: 0.1418 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0568/0938 | LOSS: 0.1423 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0569/0938 | LOSS: 0.1422 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0570/0938 | LOSS: 0.1421 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0571/0938 | LOSS: 0.1423 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0572/0938 | LOSS: 0.1422 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0573/0938 | LOSS: 0.1421 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0574/0938 | LOSS: 0.1421 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0575/0938 | LOSS: 0.1423 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0576/0938 | LOSS: 0.1424 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0577/0938 | LOSS: 0.1425 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0578/0938 | LOSS: 0.1425 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0579/0938 | LOSS: 0.1426 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0580/0938 | LOSS: 0.1426 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0581/0938 | LOSS: 0.1427 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0582/0938 | LOSS: 0.1428 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0583/0938 | LOSS: 0.1428 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0584/0938 | LOSS: 0.1428 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0585/0938 | LOSS: 0.1430 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0586/0938 | LOSS: 0.1430 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0587/0938 | LOSS: 0.1429 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0588/0938 | LOSS: 0.1430 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0589/0938 | LOSS: 0.1430 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0590/0938 | LOSS: 0.1431 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0591/0938 | LOSS: 0.1430 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0592/0938 | LOSS: 0.1430 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0593/0938 | LOSS: 0.1431 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0594/0938 | LOSS: 0.1430 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0595/0938 | LOSS: 0.1429 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0596/0938 | LOSS: 0.1430 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0597/0938 | LOSS: 0.1430 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0598/0938 | LOSS: 0.1432 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0599/0938 | LOSS: 0.1434 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0600/0938 | LOSS: 0.1434 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0601/0938 | LOSS: 0.1436 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0602/0938 | LOSS: 0.1436 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0603/0938 | LOSS: 0.1436 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0604/0938 | LOSS: 0.1437 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0605/0938 | LOSS: 0.1437 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0606/0938 | LOSS: 0.1438 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0607/0938 | LOSS: 0.1442 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0608/0938 | LOSS: 0.1443 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0609/0938 | LOSS: 0.1443 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0610/0938 | LOSS: 0.1444 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0611/0938 | LOSS: 0.1447 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0612/0938 | LOSS: 0.1446 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0613/0938 | LOSS: 0.1444 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0614/0938 | LOSS: 0.1444 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0615/0938 | LOSS: 0.1446 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0616/0938 | LOSS: 0.1447 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0617/0938 | LOSS: 0.1445 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0618/0938 | LOSS: 0.1447 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0619/0938 | LOSS: 0.1448 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0620/0938 | LOSS: 0.1446 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0621/0938 | LOSS: 0.1446 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0622/0938 | LOSS: 0.1447 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0623/0938 | LOSS: 0.1450 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0624/0938 | LOSS: 0.1452 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0625/0938 | LOSS: 0.1453 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0626/0938 | LOSS: 0.1452 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0627/0938 | LOSS: 0.1452 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0628/0938 | LOSS: 0.1451 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0629/0938 | LOSS: 0.1453 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0630/0938 | LOSS: 0.1453 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0631/0938 | LOSS: 0.1454 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0632/0938 | LOSS: 0.1454 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0633/0938 | LOSS: 0.1454 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0634/0938 | LOSS: 0.1456 | ACC 0.9581\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0635/0938 | LOSS: 0.1456 | ACC 0.9580\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0636/0938 | LOSS: 0.1460 | ACC 0.9580\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0637/0938 | LOSS: 0.1459 | ACC 0.9580\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0638/0938 | LOSS: 0.1457 | ACC 0.9580\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0639/0938 | LOSS: 0.1456 | ACC 0.9581\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0640/0938 | LOSS: 0.1456 | ACC 0.9581\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0641/0938 | LOSS: 0.1455 | ACC 0.9581\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0642/0938 | LOSS: 0.1454 | ACC 0.9581\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0643/0938 | LOSS: 0.1454 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0644/0938 | LOSS: 0.1453 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0645/0938 | LOSS: 0.1451 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0646/0938 | LOSS: 0.1450 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0647/0938 | LOSS: 0.1450 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0648/0938 | LOSS: 0.1452 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0649/0938 | LOSS: 0.1451 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0650/0938 | LOSS: 0.1452 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0651/0938 | LOSS: 0.1450 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0652/0938 | LOSS: 0.1450 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0653/0938 | LOSS: 0.1450 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0654/0938 | LOSS: 0.1452 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0655/0938 | LOSS: 0.1451 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0656/0938 | LOSS: 0.1452 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0657/0938 | LOSS: 0.1451 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0658/0938 | LOSS: 0.1450 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0659/0938 | LOSS: 0.1450 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0660/0938 | LOSS: 0.1452 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0661/0938 | LOSS: 0.1450 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0662/0938 | LOSS: 0.1451 | ACC 0.9582\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0663/0938 | LOSS: 0.1449 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0664/0938 | LOSS: 0.1449 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0665/0938 | LOSS: 0.1449 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0666/0938 | LOSS: 0.1448 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0667/0938 | LOSS: 0.1446 | ACC 0.9583\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0668/0938 | LOSS: 0.1445 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0669/0938 | LOSS: 0.1445 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0670/0938 | LOSS: 0.1444 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0671/0938 | LOSS: 0.1443 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0672/0938 | LOSS: 0.1442 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0673/0938 | LOSS: 0.1441 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0674/0938 | LOSS: 0.1442 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0675/0938 | LOSS: 0.1440 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0676/0938 | LOSS: 0.1443 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0677/0938 | LOSS: 0.1443 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0678/0938 | LOSS: 0.1441 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0679/0938 | LOSS: 0.1442 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0680/0938 | LOSS: 0.1442 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0681/0938 | LOSS: 0.1442 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0682/0938 | LOSS: 0.1441 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0683/0938 | LOSS: 0.1441 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0684/0938 | LOSS: 0.1440 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0685/0938 | LOSS: 0.1439 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0686/0938 | LOSS: 0.1438 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0687/0938 | LOSS: 0.1439 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0688/0938 | LOSS: 0.1439 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0689/0938 | LOSS: 0.1438 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0690/0938 | LOSS: 0.1438 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0691/0938 | LOSS: 0.1437 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0692/0938 | LOSS: 0.1437 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0693/0938 | LOSS: 0.1436 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0694/0938 | LOSS: 0.1435 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0695/0938 | LOSS: 0.1434 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0696/0938 | LOSS: 0.1436 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0697/0938 | LOSS: 0.1436 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0698/0938 | LOSS: 0.1435 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0699/0938 | LOSS: 0.1435 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0700/0938 | LOSS: 0.1434 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0701/0938 | LOSS: 0.1433 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0702/0938 | LOSS: 0.1432 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0703/0938 | LOSS: 0.1432 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0704/0938 | LOSS: 0.1431 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0705/0938 | LOSS: 0.1430 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0706/0938 | LOSS: 0.1428 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0707/0938 | LOSS: 0.1431 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0708/0938 | LOSS: 0.1431 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0709/0938 | LOSS: 0.1430 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0710/0938 | LOSS: 0.1429 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0711/0938 | LOSS: 0.1429 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0712/0938 | LOSS: 0.1427 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0713/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0714/0938 | LOSS: 0.1427 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0715/0938 | LOSS: 0.1428 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0716/0938 | LOSS: 0.1429 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0717/0938 | LOSS: 0.1431 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0718/0938 | LOSS: 0.1430 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0719/0938 | LOSS: 0.1429 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0720/0938 | LOSS: 0.1429 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0721/0938 | LOSS: 0.1428 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0722/0938 | LOSS: 0.1429 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0723/0938 | LOSS: 0.1428 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0724/0938 | LOSS: 0.1428 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0725/0938 | LOSS: 0.1429 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0726/0938 | LOSS: 0.1430 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0727/0938 | LOSS: 0.1431 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0728/0938 | LOSS: 0.1431 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0729/0938 | LOSS: 0.1432 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0730/0938 | LOSS: 0.1431 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0731/0938 | LOSS: 0.1430 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0732/0938 | LOSS: 0.1431 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0733/0938 | LOSS: 0.1431 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0734/0938 | LOSS: 0.1430 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0735/0938 | LOSS: 0.1430 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0736/0938 | LOSS: 0.1430 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0737/0938 | LOSS: 0.1430 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0738/0938 | LOSS: 0.1431 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0739/0938 | LOSS: 0.1431 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0740/0938 | LOSS: 0.1430 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0741/0938 | LOSS: 0.1430 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0742/0938 | LOSS: 0.1431 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0743/0938 | LOSS: 0.1430 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0744/0938 | LOSS: 0.1429 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0745/0938 | LOSS: 0.1428 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0746/0938 | LOSS: 0.1427 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0747/0938 | LOSS: 0.1426 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0748/0938 | LOSS: 0.1427 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0749/0938 | LOSS: 0.1426 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0750/0938 | LOSS: 0.1426 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0751/0938 | LOSS: 0.1425 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0752/0938 | LOSS: 0.1426 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0753/0938 | LOSS: 0.1425 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0754/0938 | LOSS: 0.1425 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0755/0938 | LOSS: 0.1424 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0756/0938 | LOSS: 0.1426 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0757/0938 | LOSS: 0.1425 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0758/0938 | LOSS: 0.1425 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0759/0938 | LOSS: 0.1425 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0760/0938 | LOSS: 0.1424 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0761/0938 | LOSS: 0.1423 | ACC 0.9589\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0762/0938 | LOSS: 0.1425 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0763/0938 | LOSS: 0.1429 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0764/0938 | LOSS: 0.1428 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0765/0938 | LOSS: 0.1427 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0766/0938 | LOSS: 0.1427 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0767/0938 | LOSS: 0.1428 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0768/0938 | LOSS: 0.1428 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0769/0938 | LOSS: 0.1428 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0770/0938 | LOSS: 0.1428 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0771/0938 | LOSS: 0.1428 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0772/0938 | LOSS: 0.1429 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0773/0938 | LOSS: 0.1428 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0774/0938 | LOSS: 0.1428 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0775/0938 | LOSS: 0.1428 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0776/0938 | LOSS: 0.1428 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0777/0938 | LOSS: 0.1427 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0778/0938 | LOSS: 0.1426 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0779/0938 | LOSS: 0.1427 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0780/0938 | LOSS: 0.1427 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0781/0938 | LOSS: 0.1426 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0782/0938 | LOSS: 0.1426 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0783/0938 | LOSS: 0.1425 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0784/0938 | LOSS: 0.1425 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0785/0938 | LOSS: 0.1427 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0786/0938 | LOSS: 0.1427 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0787/0938 | LOSS: 0.1426 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0788/0938 | LOSS: 0.1425 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0789/0938 | LOSS: 0.1424 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0790/0938 | LOSS: 0.1425 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0791/0938 | LOSS: 0.1424 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0792/0938 | LOSS: 0.1425 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0793/0938 | LOSS: 0.1425 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0794/0938 | LOSS: 0.1426 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0795/0938 | LOSS: 0.1427 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0796/0938 | LOSS: 0.1427 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0797/0938 | LOSS: 0.1426 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0798/0938 | LOSS: 0.1425 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0799/0938 | LOSS: 0.1427 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0800/0938 | LOSS: 0.1425 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0801/0938 | LOSS: 0.1428 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0802/0938 | LOSS: 0.1428 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0803/0938 | LOSS: 0.1428 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0804/0938 | LOSS: 0.1427 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0805/0938 | LOSS: 0.1427 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0806/0938 | LOSS: 0.1427 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0807/0938 | LOSS: 0.1430 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0808/0938 | LOSS: 0.1431 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0809/0938 | LOSS: 0.1433 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0810/0938 | LOSS: 0.1435 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0811/0938 | LOSS: 0.1434 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0812/0938 | LOSS: 0.1435 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0813/0938 | LOSS: 0.1434 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0814/0938 | LOSS: 0.1434 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0815/0938 | LOSS: 0.1434 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0816/0938 | LOSS: 0.1435 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0817/0938 | LOSS: 0.1436 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0818/0938 | LOSS: 0.1435 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0819/0938 | LOSS: 0.1436 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0820/0938 | LOSS: 0.1437 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0821/0938 | LOSS: 0.1436 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0822/0938 | LOSS: 0.1435 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0823/0938 | LOSS: 0.1434 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0824/0938 | LOSS: 0.1433 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0825/0938 | LOSS: 0.1432 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0826/0938 | LOSS: 0.1432 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0827/0938 | LOSS: 0.1432 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0828/0938 | LOSS: 0.1432 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0829/0938 | LOSS: 0.1432 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0830/0938 | LOSS: 0.1432 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0831/0938 | LOSS: 0.1432 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0832/0938 | LOSS: 0.1432 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0833/0938 | LOSS: 0.1431 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0834/0938 | LOSS: 0.1430 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0835/0938 | LOSS: 0.1429 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0836/0938 | LOSS: 0.1429 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0837/0938 | LOSS: 0.1428 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0838/0938 | LOSS: 0.1429 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0839/0938 | LOSS: 0.1429 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0840/0938 | LOSS: 0.1429 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0841/0938 | LOSS: 0.1429 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0842/0938 | LOSS: 0.1428 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0843/0938 | LOSS: 0.1428 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0844/0938 | LOSS: 0.1427 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0845/0938 | LOSS: 0.1427 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0846/0938 | LOSS: 0.1427 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0847/0938 | LOSS: 0.1427 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0848/0938 | LOSS: 0.1427 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0849/0938 | LOSS: 0.1427 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0850/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0851/0938 | LOSS: 0.1425 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0852/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0853/0938 | LOSS: 0.1427 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0854/0938 | LOSS: 0.1426 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0855/0938 | LOSS: 0.1427 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0856/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0857/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0858/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0859/0938 | LOSS: 0.1425 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0860/0938 | LOSS: 0.1426 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0861/0938 | LOSS: 0.1427 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0862/0938 | LOSS: 0.1426 | ACC 0.9587\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0863/0938 | LOSS: 0.1425 | ACC 0.9588\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0864/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0865/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0866/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0867/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0868/0938 | LOSS: 0.1427 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0869/0938 | LOSS: 0.1427 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0870/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0871/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0872/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0873/0938 | LOSS: 0.1425 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0874/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0875/0938 | LOSS: 0.1427 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0876/0938 | LOSS: 0.1427 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0877/0938 | LOSS: 0.1428 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0878/0938 | LOSS: 0.1429 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0879/0938 | LOSS: 0.1428 | ACC 0.9584\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0880/0938 | LOSS: 0.1427 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0881/0938 | LOSS: 0.1426 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0882/0938 | LOSS: 0.1426 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0883/0938 | LOSS: 0.1425 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0884/0938 | LOSS: 0.1423 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0885/0938 | LOSS: 0.1426 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0886/0938 | LOSS: 0.1425 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0887/0938 | LOSS: 0.1424 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0888/0938 | LOSS: 0.1424 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0889/0938 | LOSS: 0.1423 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0890/0938 | LOSS: 0.1423 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0891/0938 | LOSS: 0.1423 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0892/0938 | LOSS: 0.1423 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0893/0938 | LOSS: 0.1422 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0894/0938 | LOSS: 0.1421 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0895/0938 | LOSS: 0.1422 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0896/0938 | LOSS: 0.1422 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0897/0938 | LOSS: 0.1422 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0898/0938 | LOSS: 0.1421 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0899/0938 | LOSS: 0.1421 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0900/0938 | LOSS: 0.1422 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0901/0938 | LOSS: 0.1424 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0902/0938 | LOSS: 0.1424 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0903/0938 | LOSS: 0.1425 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0904/0938 | LOSS: 0.1425 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0905/0938 | LOSS: 0.1424 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0906/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0907/0938 | LOSS: 0.1427 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0908/0938 | LOSS: 0.1427 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0909/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0910/0938 | LOSS: 0.1425 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0911/0938 | LOSS: 0.1426 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0912/0938 | LOSS: 0.1426 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0913/0938 | LOSS: 0.1425 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0914/0938 | LOSS: 0.1425 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0915/0938 | LOSS: 0.1424 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0916/0938 | LOSS: 0.1424 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0917/0938 | LOSS: 0.1424 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0918/0938 | LOSS: 0.1423 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0919/0938 | LOSS: 0.1423 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0920/0938 | LOSS: 0.1422 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0921/0938 | LOSS: 0.1421 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0922/0938 | LOSS: 0.1421 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0923/0938 | LOSS: 0.1422 | ACC 0.9585\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0924/0938 | LOSS: 0.1423 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0925/0938 | LOSS: 0.1423 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0926/0938 | LOSS: 0.1422 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0927/0938 | LOSS: 0.1421 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0928/0938 | LOSS: 0.1420 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0929/0938 | LOSS: 0.1420 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0930/0938 | LOSS: 0.1421 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0931/0938 | LOSS: 0.1420 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0932/0938 | LOSS: 0.1420 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0933/0938 | LOSS: 0.1420 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0934/0938 | LOSS: 0.1420 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0935/0938 | LOSS: 0.1419 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0936/0938 | LOSS: 0.1420 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0937/0938 | LOSS: 0.1420 | ACC 0.9586\n",
            "TRAIN: EPOCH 0010/0010 | BATCH 0938/0938 | LOSS: 0.1420 | ACC 0.9586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# validation"
      ],
      "metadata": {
        "id": "BVBD7QjzGU_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "batch_size = 64\n",
        "num_epoch = 10\n",
        "\n",
        "ckpt_dir = './checkpoint'\n",
        "log_dir = './log'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "## 네트워크를 구축하기\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5, stride=1, padding=0, bias=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, stride=1, padding=0, bias=True)\n",
        "        self.drop2 = nn.Dropout2d(p=0.5)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=320, out_features=50, bias=True)\n",
        "        self.relu1_fc1 = nn.ReLU()\n",
        "        self.drop1_fc1 = nn.Dropout2d(p=0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(in_features=50, out_features=10, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.relu1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.drop2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.relu2(x)\n",
        "\n",
        "        x = x.view(-1, 320)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1_fc1(x)\n",
        "        x = self.drop1_fc1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "## 네트워크를 저장하거나 불러오는 함수 작성하기\n",
        "def save(ckpt_dir, net, optim, epoch):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "\n",
        "    torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},\n",
        "               './%s/model_epoch%d.pth' % (ckpt_dir, epoch))\n",
        "\n",
        "def load(ckpt_dir, net, optim):\n",
        "    ckpt_lst = os.listdir(ckpt_dir)\n",
        "    ckpt_lst.sort()\n",
        "\n",
        "    dict_model = torch.load('./%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n",
        "\n",
        "    net.load_state_dict(dict_model['net'])\n",
        "    optim.load_state_dict(dict_model['optim'])\n",
        "\n",
        "    return net, optim\n",
        "\n",
        "## MNIST 데이터 불러오기\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
        "\n",
        "dataset = datasets.MNIST(download=True, root='./', train=False, transform=transform)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "num_data = len(loader.dataset)\n",
        "num_batch = np.ceil(num_data / batch_size)\n",
        "\n",
        "## 네트워크 설정 및 필요한 손실함수 구현하기\n",
        "net = Net().to(device)\n",
        "params = net.parameters()\n",
        "\n",
        "fn_loss = nn.CrossEntropyLoss().to(device)\n",
        "fn_pred = lambda output: torch.softmax(output, dim=1)\n",
        "fn_acc = lambda pred, label: ((pred.max(dim=1)[1] == label).type(torch.float)).mean()\n",
        "\n",
        "optim = torch.optim.Adam(params, lr=lr)\n",
        "\n",
        "writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "net, optim = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
        "\n",
        "## 트레이닝 시작하기\n",
        "with torch.no_grad():\n",
        "    # net.train()\n",
        "    net.eval()\n",
        "\n",
        "    loss_arr = []\n",
        "    acc_arr = []\n",
        "\n",
        "    for batch, (input, label) in enumerate(loader, 1):\n",
        "        input = input.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        output = net(input)\n",
        "        pred = fn_pred(output)\n",
        "\n",
        "        loss = fn_loss(output, label)\n",
        "        acc = fn_acc(pred, label)\n",
        "\n",
        "        loss_arr += [loss.item()]\n",
        "        acc_arr += [acc.item()]\n",
        "\n",
        "        print('TEST: BATCH %04d/%04d | LOSS: %.4f | ACC %.4f' %\n",
        "              (batch, num_batch, np.mean(loss_arr), np.mean(acc_arr)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlHXlbKpGWSa",
        "outputId": "22acc000-3e39-4f62-f98c-3666cdeb7e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST: BATCH 0001/0157 | LOSS: 0.0030 | ACC 1.0000\n",
            "TEST: BATCH 0002/0157 | LOSS: 0.0068 | ACC 1.0000\n",
            "TEST: BATCH 0003/0157 | LOSS: 0.0067 | ACC 1.0000\n",
            "TEST: BATCH 0004/0157 | LOSS: 0.0066 | ACC 1.0000\n",
            "TEST: BATCH 0005/0157 | LOSS: 0.0108 | ACC 0.9969\n",
            "TEST: BATCH 0006/0157 | LOSS: 0.0142 | ACC 0.9948\n",
            "TEST: BATCH 0007/0157 | LOSS: 0.0177 | ACC 0.9933\n",
            "TEST: BATCH 0008/0157 | LOSS: 0.0240 | ACC 0.9922\n",
            "TEST: BATCH 0009/0157 | LOSS: 0.0245 | ACC 0.9931\n",
            "TEST: BATCH 0010/0157 | LOSS: 0.0326 | ACC 0.9922\n",
            "TEST: BATCH 0011/0157 | LOSS: 0.0333 | ACC 0.9915\n",
            "TEST: BATCH 0012/0157 | LOSS: 0.0369 | ACC 0.9883\n",
            "TEST: BATCH 0013/0157 | LOSS: 0.0346 | ACC 0.9892\n",
            "TEST: BATCH 0014/0157 | LOSS: 0.0350 | ACC 0.9888\n",
            "TEST: BATCH 0015/0157 | LOSS: 0.0436 | ACC 0.9865\n",
            "TEST: BATCH 0016/0157 | LOSS: 0.0475 | ACC 0.9854\n",
            "TEST: BATCH 0017/0157 | LOSS: 0.0469 | ACC 0.9853\n",
            "TEST: BATCH 0018/0157 | LOSS: 0.0470 | ACC 0.9844\n",
            "TEST: BATCH 0019/0157 | LOSS: 0.0457 | ACC 0.9852\n",
            "TEST: BATCH 0020/0157 | LOSS: 0.0522 | ACC 0.9820\n",
            "TEST: BATCH 0021/0157 | LOSS: 0.0536 | ACC 0.9807\n",
            "TEST: BATCH 0022/0157 | LOSS: 0.0527 | ACC 0.9808\n",
            "TEST: BATCH 0023/0157 | LOSS: 0.0514 | ACC 0.9817\n",
            "TEST: BATCH 0024/0157 | LOSS: 0.0542 | ACC 0.9805\n",
            "TEST: BATCH 0025/0157 | LOSS: 0.0524 | ACC 0.9812\n",
            "TEST: BATCH 0026/0157 | LOSS: 0.0545 | ACC 0.9808\n",
            "TEST: BATCH 0027/0157 | LOSS: 0.0554 | ACC 0.9803\n",
            "TEST: BATCH 0028/0157 | LOSS: 0.0560 | ACC 0.9799\n",
            "TEST: BATCH 0029/0157 | LOSS: 0.0541 | ACC 0.9806\n",
            "TEST: BATCH 0030/0157 | LOSS: 0.0551 | ACC 0.9802\n",
            "TEST: BATCH 0031/0157 | LOSS: 0.0537 | ACC 0.9808\n",
            "TEST: BATCH 0032/0157 | LOSS: 0.0579 | ACC 0.9790\n",
            "TEST: BATCH 0033/0157 | LOSS: 0.0591 | ACC 0.9787\n",
            "TEST: BATCH 0034/0157 | LOSS: 0.0606 | ACC 0.9784\n",
            "TEST: BATCH 0035/0157 | LOSS: 0.0608 | ACC 0.9777\n",
            "TEST: BATCH 0036/0157 | LOSS: 0.0607 | ACC 0.9779\n",
            "TEST: BATCH 0037/0157 | LOSS: 0.0597 | ACC 0.9780\n",
            "TEST: BATCH 0038/0157 | LOSS: 0.0600 | ACC 0.9782\n",
            "TEST: BATCH 0039/0157 | LOSS: 0.0633 | ACC 0.9776\n",
            "TEST: BATCH 0040/0157 | LOSS: 0.0619 | ACC 0.9781\n",
            "TEST: BATCH 0041/0157 | LOSS: 0.0639 | ACC 0.9783\n",
            "TEST: BATCH 0042/0157 | LOSS: 0.0650 | ACC 0.9784\n",
            "TEST: BATCH 0043/0157 | LOSS: 0.0637 | ACC 0.9789\n",
            "TEST: BATCH 0044/0157 | LOSS: 0.0628 | ACC 0.9790\n",
            "TEST: BATCH 0045/0157 | LOSS: 0.0618 | ACC 0.9795\n",
            "TEST: BATCH 0046/0157 | LOSS: 0.0626 | ACC 0.9786\n",
            "TEST: BATCH 0047/0157 | LOSS: 0.0617 | ACC 0.9791\n",
            "TEST: BATCH 0048/0157 | LOSS: 0.0608 | ACC 0.9795\n",
            "TEST: BATCH 0049/0157 | LOSS: 0.0608 | ACC 0.9796\n",
            "TEST: BATCH 0050/0157 | LOSS: 0.0597 | ACC 0.9800\n",
            "TEST: BATCH 0051/0157 | LOSS: 0.0589 | ACC 0.9801\n",
            "TEST: BATCH 0052/0157 | LOSS: 0.0587 | ACC 0.9799\n",
            "TEST: BATCH 0053/0157 | LOSS: 0.0579 | ACC 0.9802\n",
            "TEST: BATCH 0054/0157 | LOSS: 0.0579 | ACC 0.9800\n",
            "TEST: BATCH 0055/0157 | LOSS: 0.0578 | ACC 0.9801\n",
            "TEST: BATCH 0056/0157 | LOSS: 0.0606 | ACC 0.9799\n",
            "TEST: BATCH 0057/0157 | LOSS: 0.0601 | ACC 0.9800\n",
            "TEST: BATCH 0058/0157 | LOSS: 0.0594 | ACC 0.9801\n",
            "TEST: BATCH 0059/0157 | LOSS: 0.0623 | ACC 0.9793\n",
            "TEST: BATCH 0060/0157 | LOSS: 0.0634 | ACC 0.9792\n",
            "TEST: BATCH 0061/0157 | LOSS: 0.0630 | ACC 0.9790\n",
            "TEST: BATCH 0062/0157 | LOSS: 0.0632 | ACC 0.9791\n",
            "TEST: BATCH 0063/0157 | LOSS: 0.0630 | ACC 0.9792\n",
            "TEST: BATCH 0064/0157 | LOSS: 0.0625 | ACC 0.9792\n",
            "TEST: BATCH 0065/0157 | LOSS: 0.0617 | ACC 0.9796\n",
            "TEST: BATCH 0066/0157 | LOSS: 0.0619 | ACC 0.9792\n",
            "TEST: BATCH 0067/0157 | LOSS: 0.0626 | ACC 0.9788\n",
            "TEST: BATCH 0068/0157 | LOSS: 0.0618 | ACC 0.9791\n",
            "TEST: BATCH 0069/0157 | LOSS: 0.0615 | ACC 0.9789\n",
            "TEST: BATCH 0070/0157 | LOSS: 0.0609 | ACC 0.9792\n",
            "TEST: BATCH 0071/0157 | LOSS: 0.0610 | ACC 0.9793\n",
            "TEST: BATCH 0072/0157 | LOSS: 0.0604 | ACC 0.9796\n",
            "TEST: BATCH 0073/0157 | LOSS: 0.0599 | ACC 0.9799\n",
            "TEST: BATCH 0074/0157 | LOSS: 0.0592 | ACC 0.9802\n",
            "TEST: BATCH 0075/0157 | LOSS: 0.0589 | ACC 0.9802\n",
            "TEST: BATCH 0076/0157 | LOSS: 0.0592 | ACC 0.9801\n",
            "TEST: BATCH 0077/0157 | LOSS: 0.0590 | ACC 0.9799\n",
            "TEST: BATCH 0078/0157 | LOSS: 0.0589 | ACC 0.9800\n",
            "TEST: BATCH 0079/0157 | LOSS: 0.0581 | ACC 0.9802\n",
            "TEST: BATCH 0080/0157 | LOSS: 0.0576 | ACC 0.9803\n",
            "TEST: BATCH 0081/0157 | LOSS: 0.0571 | ACC 0.9805\n",
            "TEST: BATCH 0082/0157 | LOSS: 0.0564 | ACC 0.9808\n",
            "TEST: BATCH 0083/0157 | LOSS: 0.0558 | ACC 0.9810\n",
            "TEST: BATCH 0084/0157 | LOSS: 0.0551 | ACC 0.9812\n",
            "TEST: BATCH 0085/0157 | LOSS: 0.0545 | ACC 0.9814\n",
            "TEST: BATCH 0086/0157 | LOSS: 0.0539 | ACC 0.9816\n",
            "TEST: BATCH 0087/0157 | LOSS: 0.0532 | ACC 0.9819\n",
            "TEST: BATCH 0088/0157 | LOSS: 0.0527 | ACC 0.9821\n",
            "TEST: BATCH 0089/0157 | LOSS: 0.0521 | ACC 0.9823\n",
            "TEST: BATCH 0090/0157 | LOSS: 0.0517 | ACC 0.9823\n",
            "TEST: BATCH 0091/0157 | LOSS: 0.0512 | ACC 0.9825\n",
            "TEST: BATCH 0092/0157 | LOSS: 0.0508 | ACC 0.9827\n",
            "TEST: BATCH 0093/0157 | LOSS: 0.0509 | ACC 0.9827\n",
            "TEST: BATCH 0094/0157 | LOSS: 0.0512 | ACC 0.9824\n",
            "TEST: BATCH 0095/0157 | LOSS: 0.0510 | ACC 0.9826\n",
            "TEST: BATCH 0096/0157 | LOSS: 0.0506 | ACC 0.9827\n",
            "TEST: BATCH 0097/0157 | LOSS: 0.0503 | ACC 0.9828\n",
            "TEST: BATCH 0098/0157 | LOSS: 0.0497 | ACC 0.9829\n",
            "TEST: BATCH 0099/0157 | LOSS: 0.0492 | ACC 0.9831\n",
            "TEST: BATCH 0100/0157 | LOSS: 0.0488 | ACC 0.9833\n",
            "TEST: BATCH 0101/0157 | LOSS: 0.0483 | ACC 0.9834\n",
            "TEST: BATCH 0102/0157 | LOSS: 0.0485 | ACC 0.9835\n",
            "TEST: BATCH 0103/0157 | LOSS: 0.0491 | ACC 0.9832\n",
            "TEST: BATCH 0104/0157 | LOSS: 0.0508 | ACC 0.9829\n",
            "TEST: BATCH 0105/0157 | LOSS: 0.0503 | ACC 0.9830\n",
            "TEST: BATCH 0106/0157 | LOSS: 0.0502 | ACC 0.9830\n",
            "TEST: BATCH 0107/0157 | LOSS: 0.0500 | ACC 0.9831\n",
            "TEST: BATCH 0108/0157 | LOSS: 0.0497 | ACC 0.9832\n",
            "TEST: BATCH 0109/0157 | LOSS: 0.0492 | ACC 0.9834\n",
            "TEST: BATCH 0110/0157 | LOSS: 0.0488 | ACC 0.9835\n",
            "TEST: BATCH 0111/0157 | LOSS: 0.0484 | ACC 0.9837\n",
            "TEST: BATCH 0112/0157 | LOSS: 0.0480 | ACC 0.9838\n",
            "TEST: BATCH 0113/0157 | LOSS: 0.0477 | ACC 0.9840\n",
            "TEST: BATCH 0114/0157 | LOSS: 0.0473 | ACC 0.9841\n",
            "TEST: BATCH 0115/0157 | LOSS: 0.0469 | ACC 0.9842\n",
            "TEST: BATCH 0116/0157 | LOSS: 0.0465 | ACC 0.9844\n",
            "TEST: BATCH 0117/0157 | LOSS: 0.0462 | ACC 0.9845\n",
            "TEST: BATCH 0118/0157 | LOSS: 0.0458 | ACC 0.9846\n",
            "TEST: BATCH 0119/0157 | LOSS: 0.0454 | ACC 0.9848\n",
            "TEST: BATCH 0120/0157 | LOSS: 0.0451 | ACC 0.9849\n",
            "TEST: BATCH 0121/0157 | LOSS: 0.0447 | ACC 0.9850\n",
            "TEST: BATCH 0122/0157 | LOSS: 0.0444 | ACC 0.9851\n",
            "TEST: BATCH 0123/0157 | LOSS: 0.0443 | ACC 0.9851\n",
            "TEST: BATCH 0124/0157 | LOSS: 0.0441 | ACC 0.9853\n",
            "TEST: BATCH 0125/0157 | LOSS: 0.0437 | ACC 0.9854\n",
            "TEST: BATCH 0126/0157 | LOSS: 0.0435 | ACC 0.9855\n",
            "TEST: BATCH 0127/0157 | LOSS: 0.0435 | ACC 0.9855\n",
            "TEST: BATCH 0128/0157 | LOSS: 0.0432 | ACC 0.9856\n",
            "TEST: BATCH 0129/0157 | LOSS: 0.0429 | ACC 0.9857\n",
            "TEST: BATCH 0130/0157 | LOSS: 0.0429 | ACC 0.9857\n",
            "TEST: BATCH 0131/0157 | LOSS: 0.0426 | ACC 0.9858\n",
            "TEST: BATCH 0132/0157 | LOSS: 0.0424 | ACC 0.9859\n",
            "TEST: BATCH 0133/0157 | LOSS: 0.0421 | ACC 0.9860\n",
            "TEST: BATCH 0134/0157 | LOSS: 0.0419 | ACC 0.9860\n",
            "TEST: BATCH 0135/0157 | LOSS: 0.0416 | ACC 0.9861\n",
            "TEST: BATCH 0136/0157 | LOSS: 0.0413 | ACC 0.9862\n",
            "TEST: BATCH 0137/0157 | LOSS: 0.0410 | ACC 0.9863\n",
            "TEST: BATCH 0138/0157 | LOSS: 0.0407 | ACC 0.9864\n",
            "TEST: BATCH 0139/0157 | LOSS: 0.0404 | ACC 0.9865\n",
            "TEST: BATCH 0140/0157 | LOSS: 0.0401 | ACC 0.9866\n",
            "TEST: BATCH 0141/0157 | LOSS: 0.0410 | ACC 0.9864\n",
            "TEST: BATCH 0142/0157 | LOSS: 0.0411 | ACC 0.9862\n",
            "TEST: BATCH 0143/0157 | LOSS: 0.0408 | ACC 0.9863\n",
            "TEST: BATCH 0144/0157 | LOSS: 0.0406 | ACC 0.9864\n",
            "TEST: BATCH 0145/0157 | LOSS: 0.0403 | ACC 0.9865\n",
            "TEST: BATCH 0146/0157 | LOSS: 0.0400 | ACC 0.9866\n",
            "TEST: BATCH 0147/0157 | LOSS: 0.0398 | ACC 0.9867\n",
            "TEST: BATCH 0148/0157 | LOSS: 0.0395 | ACC 0.9868\n",
            "TEST: BATCH 0149/0157 | LOSS: 0.0393 | ACC 0.9869\n",
            "TEST: BATCH 0150/0157 | LOSS: 0.0391 | ACC 0.9870\n",
            "TEST: BATCH 0151/0157 | LOSS: 0.0392 | ACC 0.9870\n",
            "TEST: BATCH 0152/0157 | LOSS: 0.0395 | ACC 0.9867\n",
            "TEST: BATCH 0153/0157 | LOSS: 0.0401 | ACC 0.9867\n",
            "TEST: BATCH 0154/0157 | LOSS: 0.0400 | ACC 0.9868\n",
            "TEST: BATCH 0155/0157 | LOSS: 0.0399 | ACC 0.9869\n",
            "TEST: BATCH 0156/0157 | LOSS: 0.0399 | ACC 0.9869\n",
            "TEST: BATCH 0157/0157 | LOSS: 0.0396 | ACC 0.9870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYSoRqUdH8bG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
